{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f53e0774",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "43875e59",
   "metadata": {},
   "source": [
    "### **Traducción del Texto:**\n",
    "**El Perceptrón**  \n",
    "El Perceptrón es una de las arquitecturas más simples de redes neuronales artificiales (RNA), inventada en 1957 por Frank Rosenblatt. Se basa en una neurona artificial ligeramente diferente (ver Figura 10-4) llamada **unidad de lógica de umbral (TLU)** o, a veces, **unidad lineal de umbral (LTU)**:  \n",
    "- Las entradas y salidas son números (en lugar de valores binarios \"on/off\"). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22124dc5",
   "metadata": {},
   "source": [
    " \n",
    "- Cada conexión de entrada tiene un **peso asociado**.  \n",
    "- La TLU calcula una **suma ponderada** de sus entradas:  \n",
    "  $$\n",
    "  z = w_1 x_1 + w_2 x_2 + \\dots + w_n x_n = \\mathbf{x}^T \\mathbf{w}\n",
    "  $$  \n",
    "  Luego aplica una **función escalón** a esta suma y devuelve el resultado:  \n",
    "  $$\n",
    "  h_{\\mathbf{w}}(\\mathbf{x}) = \\text{step}(z)\n",
    "  $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074fde60",
   "metadata": {},
   "source": [
    "\n",
    "**Funciones escalón comunes**:  \n",
    "- **Función de Heaviside**:  \n",
    "  $$\n",
    "  \\text{heaviside}(z) = \\begin{cases}\n",
    "  0 & \\text{si } z < 0 \\\\\n",
    "  1 & \\text{si } z \\geq 0\n",
    "  \\end{cases}\n",
    "  $$  \n",
    "- **Función signo**:  \n",
    "  $$\n",
    "  \\text{sgn}(z) = \\begin{cases}\n",
    "  -1 & \\text{si } z < 0 \\\\\n",
    "  0 & \\text{si } z = 0 \\\\\n",
    "  +1 & \\text{si } z > 0\n",
    "  \\end{cases}\n",
    "  $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a9ea05",
   "metadata": {},
   "source": [
    "\n",
    "**Aplicación**:  \n",
    "- Una sola TLU puede realizar **clasificación binaria lineal** (similar a Regresión Logística o SVM lineal).  \n",
    "- Ejemplo: Clasificar flores iris basándose en longitud y ancho del pétalo, usando un sesgo ($x_0 = 1$).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afa8d48",
   "metadata": {},
   "source": [
    "\n",
    "**Perceptrón multicapa**:  \n",
    "- Un Perceptrón es una **capa densa (fully connected)** de TLUs, donde cada neurona está conectada a todas las entradas.  \n",
    "- Se entrena con una variante de la **Regla de Hebb**: ajusta los pesos para reducir el error en las predicciones (ver Ecuación 10-3).  \n",
    "- **Limitación**: No puede resolver problemas no lineales (como XOR), pero un **MLP (Multi-Layer Perceptron)** sí puede.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c666232b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### **Diseño de Práctica de Codificación en Python**\n",
    "\n",
    "#### **Objetivo**:  \n",
    "Implementar un Perceptrón desde cero para clasificación binaria, usando la función de Heaviside, y aplicarlo al problema de XOR con un MLP.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcac98ab",
   "metadata": {},
   "source": [
    "\n",
    "#### **Ejercicio 1: Implementar una TLU (Threshold Logic Unit)**\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "def heaviside(z):\n",
    "    return np.where(z >= 0, 1, 0)  # Función de Heaviside\n",
    "\n",
    "class TLU:\n",
    "    def __init__(self, input_size):\n",
    "        self.weights = np.random.randn(input_size)\n",
    "        self.bias = np.random.randn()\n",
    "    \n",
    "    def predict(self, X):\n",
    "        z = np.dot(X, self.weights) + self.bias\n",
    "        return heaviside(z)\n",
    "```\n",
    "\n",
    "**Prueba**:  \n",
    "```python\n",
    "tlu = TLU(2)\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "print(\"Predicciones TLU aleatorias:\", tlu.predict(X))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **Ejercicio 2: Entrenar un Perceptrón (Regla de Rosenblatt)**\n",
    "```python\n",
    "class Perceptron:\n",
    "    def __init__(self, input_size, lr=0.1):\n",
    "        self.weights = np.zeros(input_size)\n",
    "        self.bias = 0\n",
    "        self.lr = lr  # Tasa de aprendizaje\n",
    "    \n",
    "    def fit(self, X, y, epochs=100):\n",
    "        for _ in range(epochs):\n",
    "            for xi, yi in zip(X, y):\n",
    "                y_pred = heaviside(np.dot(xi, self.weights) + self.bias)\n",
    "                error = yi - y_pred\n",
    "                self.weights += self.lr * error * xi\n",
    "                self.bias += self.lr * error\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return heaviside(np.dot(X, self.weights) + self.bias)\n",
    "```\n",
    "\n",
    "**Prueba con OR**:  \n",
    "```python\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([0, 1, 1, 1])  # OR\n",
    "p = Perceptron(input_size=2)\n",
    "p.fit(X, y)\n",
    "print(\"Predicciones OR:\", p.predict(X))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **Ejercicio 3: Resolver XOR con un MLP (2 capas)**\n",
    "```python\n",
    "# Capa 1: 2 TLUs (entrada -> oculta)\n",
    "weights1 = np.array([[1, -1], [-1, 1]])  # Pesos manuales para XOR\n",
    "bias1 = np.array([0, 0])\n",
    "\n",
    "# Capa 2: 1 TLU (oculta -> salida)\n",
    "weights2 = np.array([1, 1]])\n",
    "bias2 = -1\n",
    "\n",
    "def mlp_xor(X):\n",
    "    hidden = heaviside(np.dot(X, weights1) + bias1)\n",
    "    return heaviside(np.dot(hidden, weights2.T) + bias2)\n",
    "\n",
    "X_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "print(\"Predicciones XOR:\", mlp_xor(X_xor))  # Debe devolver [0, 1, 1, 0]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **Ejercicio 4: Comparar con Scikit-Learn**\n",
    "```python\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "# Perceptrón de una capa (falla en XOR)\n",
    "sk_p = Perceptron()\n",
    "sk_p.fit(X_xor, np.array([0, 1, 1, 0]))\n",
    "print(\"Scikit-Learn (XOR):\", sk_p.predict(X_xor))  # No resuelve XOR\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Conclusión**:  \n",
    "- El Perceptrón simple es efectivo para problemas linealmente separables.  \n",
    "- Para problemas no lineales (como XOR), se necesita un **MLP** con al menos una capa oculta.  \n",
    "- La práctica cubre desde la implementación básica hasta la combinación de capas.  \n",
    "\n",
    "**Extensión**: Modificar el MLP para usar aprendizaje automático (backpropagation) en lugar de pesos fijos."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
