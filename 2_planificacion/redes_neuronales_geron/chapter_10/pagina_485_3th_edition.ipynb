{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e193b608",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/marco-canas/ml_intro/blob/main/2_planificacion/redes_neuronales_geron/chapter_10/pagina_479_edition_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://kaggle.com/kernels/welcome?src=https://github.com/marco-canas/ml_intro/blob/main/2_planificacion/redes_neuronales_geron/chapter_10/pagina_479_edition_3.ipynb\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" /></a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95106955",
   "metadata": {},
   "source": [
    "Aquí tienes la traducción al español del texto del capítulo 10 de *Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow* de Aurélien Géron:\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163625df",
   "metadata": {},
   "source": [
    "\n",
    "# **MLPs para Regresión**  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3612b45d",
   "metadata": {},
   "source": [
    "\n",
    "En primer lugar, las redes neuronales multicapa (*MLPs*) pueden utilizarse para tareas de regresión. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbf9ec1",
   "metadata": {},
   "source": [
    "Si deseas predecir un único valor (por ejemplo, el precio de una casa dadas varias de sus características), solo necesitas una neurona de salida: su resultado será el valor predicho. Para regresión multivariante (es decir, predecir múltiples valores a la vez), necesitas una neurona de salida por cada dimensión de salida. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ea220b",
   "metadata": {},
   "source": [
    "Por ejemplo, para localizar el centro de un objeto en una imagen, necesitas predecir coordenadas en 2D, por lo que requieres dos neuronas de salida. Si además quieres dibujar un *bounding box* (rectángulo delimitador) alrededor del objeto, necesitas dos números más: el ancho y el alto del objeto. Así, terminarías con cuatro neuronas de salida.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ec6f3e",
   "metadata": {},
   "source": [
    "\n",
    "Scikit-Learn incluye la clase **`MLPRegressor`**, así que vamos a usarla para construir una MLP con tres capas ocultas de 50 neuronas cada una y entrenarla en el conjunto de datos de viviendas de California. Para simplificar, utilizaremos la función **`fetch_california_housing()`** de Scikit-Learn para cargar los datos. Este conjunto es más simple que el que usamos en el Capítulo 2, ya que solo contiene características numéricas (no incluye la característica *ocean_proximity*) y no tiene valores faltantes.  \n",
    "\n",
    "El siguiente código comienza cargando y dividiendo el conjunto de datos, luego crea un *pipeline* para estandarizar las características de entrada antes de pasarlas al **`MLPRegressor`**. Esto es muy importante para las redes neuronales, ya que se entrenan mediante *gradiente descendente*, y como vimos en el Capítulo 4, el gradiente descendente no converge bien cuando las características tienen escalas muy diferentes.  \n",
    "\n",
    "Finalmente, el código entrena el modelo y evalúa su error de validación. El modelo usa la función de activación **ReLU** en las capas ocultas y una variante del gradiente descendente llamada **Adam** (ver Capítulo 11) para minimizar el *error cuadrático medio (MSE)*, con un poco de regularización **ℓ₂** (controlable mediante el hiperparámetro **`alpha`**):  \n",
    "\n",
    "```python\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    housing.data, housing.target, random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full, y_train_full, random_state=42)\n",
    "\n",
    "mlp_reg = MLPRegressor(hidden_layer_sizes=[50, 50, 50], random_state=42)\n",
    "pipeline = make_pipeline(StandardScaler(), mlp_reg)\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "y_pred = pipeline.predict(X_valid)\n",
    "rmse = mean_squared_error(y_valid, y_pred, squared=False)  # ~0.505\n",
    "```  \n",
    "\n",
    "Obtenemos un **RMSE de validación** de aproximadamente **0.505**, comparable al rendimiento de un *Random Forest*. ¡Nada mal para un primer intento!  \n",
    "\n",
    "Cabe destacar que esta MLP **no usa ninguna función de activación en la capa de salida**, por lo que puede generar cualquier valor. Esto suele ser aceptable, pero si necesitas garantizar que la salida siempre sea positiva, deberías usar **ReLU** o **softplus** (una variante suave de ReLU: *softplus(z) = log(1 + exp(z))*). La función softplus es cercana a 0 cuando *z* es negativo y cercana a *z* cuando es positivo.  \n",
    "\n",
    "Por último, si quieres asegurar que las predicciones estén dentro de un rango específico, puedes usar **sigmoid** (para valores entre 0 y 1) o **tanh** (entre –1 y 1) y escalar los objetivos acordemente. Lamentablemente, **`MLPRegressor` no soporta funciones de activación en la capa de salida**.  \n",
    "\n",
    "⚠ **Advertencia**  \n",
    "Construir y entrenar una MLP estándar con Scikit-Learn en pocas líneas de código es muy conveniente, pero sus capacidades son limitadas. Por eso, más adelante en este capítulo pasaremos a **Keras**.  \n",
    "\n",
    "La clase **`MLPRegressor`** usa el *error cuadrático medio (MSE)*, que suele ser adecuado para regresión. Sin embargo, si hay muchos *outliers* en los datos, podrías preferir el *error absoluto medio (MAE)* o la **pérdida de Huber**, que combina MSE y MAE: es cuadrática para errores menores que un umbral **δ** (típicamente 1) y lineal para errores mayores. La parte lineal la hace menos sensible a *outliers* que el MSE, mientras que la parte cuadrática permite una convergencia más rápida que el MAE. No obstante, **`MLPRegressor` solo soporta MSE**.  \n",
    "\n",
    "La **Tabla 10-1** resume la arquitectura típica de una MLP para regresión.  \n",
    "\n",
    "| **Hiperparámetro**         | **Valor típico**                                                                 |\n",
    "|----------------------------|----------------------------------------------------------------------------------|\n",
    "| N° de capas ocultas        | Depende del problema (usualmente 1 a 5)                                          |\n",
    "| N° de neuronas por capa    | Depende del problema (usualmente 10 a 100)                                       |\n",
    "| N° de neuronas de salida   | 1 por dimensión de predicción                                                    |\n",
    "| Activación en capas ocultas| ReLU                                                                             |\n",
    "| Activación en salida       | Ninguna, ReLU/softplus (salidas positivas) o sigmoid/tanh (salidas acotadas)     |\n",
    "| Función de pérdida         | MSE, o Huber si hay *outliers*                                                   |\n",
    "\n",
    "--- \n",
    "\n",
    "Espero que esta traducción te sea útil. Si necesitas ajustes o más detalles, ¡avísame!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9691c2",
   "metadata": {},
   "source": [
    "Aquí tienes una **secuencia didáctica** para practicar la implementación de **MLPs para regresión** en Python, basada en el texto anterior. La secuencia está diseñada para avanzar desde conceptos básicos hasta técnicas más avanzadas, con ejercicios prácticos en cada etapa.\n",
    "\n",
    "---\n",
    "\n",
    "## **Secuencia Didáctica: MLPs para Regresión en Python**  \n",
    "**Objetivo:** Implementar y optimizar una red neuronal multicapa (*MLP*) para problemas de regresión usando `scikit-learn` y `Keras`.  \n",
    "\n",
    "### **1. Introducción a los Datos y Preprocesamiento**  \n",
    "**Objetivo:** Familiarizarse con el dataset y preparar los datos para el modelo.  \n",
    "\n",
    "**Ejercicios:**  \n",
    "1. **Cargar y explorar el dataset**  \n",
    "   ```python\n",
    "   from sklearn.datasets import fetch_california_housing\n",
    "   import pandas as pd\n",
    "\n",
    "   housing = fetch_california_housing()\n",
    "   df = pd.DataFrame(housing.data, columns=housing.feature_names)\n",
    "   df[\"Target\"] = housing.target\n",
    "   print(df.head())\n",
    "   print(df.describe())\n",
    "   ```\n",
    "\n",
    "2. **División del dataset (train/validation/test)**  \n",
    "   ```python\n",
    "   from sklearn.model_selection import train_test_split\n",
    "\n",
    "   X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "       housing.data, housing.target, random_state=42\n",
    "   )\n",
    "   X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "       X_train_full, y_train_full, random_state=42\n",
    "   )\n",
    "   ```\n",
    "\n",
    "3. **Estandarización de características**  \n",
    "   ```python\n",
    "   from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "   scaler = StandardScaler()\n",
    "   X_train_scaled = scaler.fit_transform(X_train)\n",
    "   X_valid_scaled = scaler.transform(X_valid)\n",
    "   X_test_scaled = scaler.transform(X_test)\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Implementación Básica con `MLPRegressor` (Scikit-Learn)**  \n",
    "**Objetivo:** Entrenar un primer modelo de MLP y evaluar su rendimiento.  \n",
    "\n",
    "**Ejercicios:**  \n",
    "1. **Entrenar un MLP básico**  \n",
    "   ```python\n",
    "   from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "   mlp_reg = MLPRegressor(hidden_layer_sizes=[50, 50, 50], random_state=42)\n",
    "   mlp_reg.fit(X_train_scaled, y_train)\n",
    "   ```\n",
    "\n",
    "2. **Evaluar el modelo (RMSE)**  \n",
    "   ```python\n",
    "   from sklearn.metrics import mean_squared_error\n",
    "\n",
    "   y_pred = mlp_reg.predict(X_valid_scaled)\n",
    "   rmse = mean_squared_error(y_valid, y_pred, squared=False)\n",
    "   print(f\"RMSE: {rmse:.3f}\")  # Debería ser ~0.505\n",
    "   ```\n",
    "\n",
    "3. **Experimentar con diferentes arquitecturas**  \n",
    "   - Probar con más/menos capas ocultas.  \n",
    "   - Cambiar el número de neuronas (ej. `[100, 50]`).  \n",
    "   - Observar cómo afecta al rendimiento.  \n",
    "\n",
    "---\n",
    "\n",
    "### **3. Optimización del Modelo**  \n",
    "**Objetivo:** Mejorar el rendimiento del MLP ajustando hiperparámetros.  \n",
    "\n",
    "**Ejercicios:**  \n",
    "1. **Ajustar la tasa de aprendizaje y el optimizador**  \n",
    "   ```python\n",
    "   mlp_reg = MLPRegressor(\n",
    "       hidden_layer_sizes=[50, 50, 50],\n",
    "       activation=\"relu\",\n",
    "       solver=\"adam\",  # Usar Adam en lugar de SGD\n",
    "       learning_rate_init=0.001,  # Tasa de aprendizaje más baja\n",
    "       max_iter=500,  # Más épocas\n",
    "       random_state=42\n",
    "   )\n",
    "   mlp_reg.fit(X_train_scaled, y_train)\n",
    "   ```\n",
    "\n",
    "2. **Regularización (L2) para evitar overfitting**  \n",
    "   ```python\n",
    "   mlp_reg = MLPRegressor(\n",
    "       hidden_layer_sizes=[50, 50, 50],\n",
    "       alpha=0.01,  # Factor de regularización L2\n",
    "       random_state=42\n",
    "   )\n",
    "   mlp_reg.fit(X_train_scaled, y_train)\n",
    "   ```\n",
    "\n",
    "3. **Early Stopping (usando `validation_fraction`)**  \n",
    "   ```python\n",
    "   mlp_reg = MLPRegressor(\n",
    "       hidden_layer_sizes=[50, 50, 50],\n",
    "       early_stopping=True,  # Detener si no mejora\n",
    "       validation_fraction=0.2,  # 20% de validación\n",
    "       random_state=42\n",
    "   )\n",
    "   mlp_reg.fit(X_train_scaled, y_train)\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Implementación con Keras (TensorFlow) para Mayor Flexibilidad**  \n",
    "**Objetivo:** Usar Keras para modelos más personalizables (ej. activaciones en la capa de salida).  \n",
    "\n",
    "**Ejercicios:**  \n",
    "1. **Crear un MLP con Keras**  \n",
    "   ```python\n",
    "   from tensorflow import keras\n",
    "\n",
    "   model = keras.Sequential([\n",
    "       keras.layers.Dense(50, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
    "       keras.layers.Dense(50, activation=\"relu\"),\n",
    "       keras.layers.Dense(50, activation=\"relu\"),\n",
    "       keras.layers.Dense(1)  # Sin activación (regresión)\n",
    "   ])\n",
    "\n",
    "   model.compile(loss=\"mse\", optimizer=\"adam\", metrics=[\"mse\"])\n",
    "   ```\n",
    "\n",
    "2. **Entrenar y evaluar el modelo**  \n",
    "   ```python\n",
    "   history = model.fit(\n",
    "       X_train_scaled, y_train,\n",
    "       epochs=50,\n",
    "       validation_data=(X_valid_scaled, y_valid)\n",
    "   )\n",
    "\n",
    "   y_pred = model.predict(X_test_scaled)\n",
    "   rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "   print(f\"RMSE (Keras): {rmse:.3f}\")\n",
    "   ```\n",
    "\n",
    "3. **Probar diferentes funciones de activación en la salida**  \n",
    "   - **ReLU/Softplus** (si la salida debe ser positiva):  \n",
    "     ```python\n",
    "     keras.layers.Dense(1, activation=\"softplus\")\n",
    "     ```\n",
    "   - **Sigmoid/Tanh** (si la salida debe estar acotada):  \n",
    "     ```python\n",
    "     keras.layers.Dense(1, activation=\"sigmoid\")  # Escalar 'y' entre 0 y 1\n",
    "     ```\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Experimentación Avanzada (Opcional)**  \n",
    "**Objetivo:** Profundizar en técnicas avanzadas para mejorar el modelo.  \n",
    "\n",
    "**Ejercicios:**  \n",
    "1. **Usar Batch Normalization**  \n",
    "   ```python\n",
    "   model.add(keras.layers.BatchNormalization())\n",
    "   ```\n",
    "2. **Probar Dropout para regularización**  \n",
    "   ```python\n",
    "   model.add(keras.layers.Dropout(0.3))\n",
    "   ```\n",
    "3. **Optimización con GridSearchCV (Scikit-Learn)**  \n",
    "   ```python\n",
    "   from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "   param_grid = {\n",
    "       \"hidden_layer_sizes\": [(50,), (100, 50)],\n",
    "       \"alpha\": [0.0001, 0.001, 0.01],\n",
    "   }\n",
    "   grid_search = GridSearchCV(MLPRegressor(), param_grid, cv=3)\n",
    "   grid_search.fit(X_train_scaled, y_train)\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "## **Resumen de la Secuencia**  \n",
    "1. **Preprocesamiento**: Estandarización y división de datos.  \n",
    "2. **Primer modelo con Scikit-Learn**: Entrenamiento y evaluación básica.  \n",
    "3. **Optimización**: Ajuste de hiperparámetros y regularización.  \n",
    "4. **Keras**: Modelos más flexibles y personalizables.  \n",
    "5. **Avanzado**: BatchNorm, Dropout y búsqueda de hiperparámetros.  \n",
    "\n",
    "Esta secuencia permite pasar de **conceptos básicos** a **técnicas avanzadas**, con ejercicios prácticos en cada paso. ¿Quieres que profundicemos en algún tema en particular?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67277d2",
   "metadata": {},
   "source": [
    "Aquí tienes **10 referentes clave** en *deep learning* para enriquecer tu docencia e investigación en matemáticas, ciencia de datos y métodos cuanti/cuali en la Universidad de Antioquia (Seccional Bajo Cauca). Estos recursos incluyen cursos, libros, investigadores y enfoques innovadores, basados en las tendencias actuales y futuras del campo :\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Cursos y Programas Académicos**  \n",
    "- **Curso \"Bases Matemáticas del Deep Learning\" (TECH Universidad Tecnológica)**:  \n",
    "  Enfocado en fundamentos matemáticos (álgebra lineal, optimización, backpropagation) y aplicaciones prácticas con metodología *Relearning*. Ideal para docencia en matemáticas aplicadas .  \n",
    "- **Track en Ciencia de Datos (Colegio Bourbaki)**:  \n",
    "  Cubre desde probabilidad bayesiana hasta redes neuronales avanzadas (CNNs, Transformers, GANs). Incluye proyectos prácticos y evaluación basada en problemas reales .  \n",
    "\n",
    "### **2. Investigadores y Expertos**  \n",
    "- **Geoffrey Hinton & Yann LeCun**:  \n",
    "  Pioneros en redes neuronales y aprendizaje no supervisado. Explora sus trabajos recientes en *capsule networks* y autoaprendizaje (*self-supervised learning*) .  \n",
    "- **Gary Marcus**:  \n",
    "  Crítico del *deep learning* puro; promueve modelos híbridos (neuro-simbólicos) que integran razonamiento abstracto y conocimiento previo. Útil para discutir limitaciones del enfoque actual .  \n",
    "\n",
    "### **3. Frameworks y Herramientas**  \n",
    "- **TensorFlow/Keras y PyTorch**:  \n",
    "  Los más usados en docencia. PyTorch es ideal para investigación por su flexibilidad en redes personalizadas .  \n",
    "- **Libro: \"Deep Learning\" (Ian Goodfellow et al.)**:  \n",
    "  Biblia técnica que cubre fundamentos matemáticos (gradientes, regularización) y arquitecturas avanzadas (GANs, RNNs) .  \n",
    "\n",
    "### **4. Tendencias Futuras**  \n",
    "- **Aprendizaje Autosupervisado (*Self-Supervised Learning*)**:  \n",
    "  Reducción de dependencia de datos etiquetados. Ejemplo: modelos de lenguaje como BERT/GPT .  \n",
    "- **Redes Generativas (GANs y Difusión Estable)**:  \n",
    "  Aplicaciones en generación de imágenes y datos sintéticos para investigación cualitativa .  \n",
    "\n",
    "### **5. Métodos Híbridos para Investigación**  \n",
    "- **Física + Deep Learning**:  \n",
    "  Modelos que integran leyes físicas en redes neuronales (ej.: predicción climática). Relevante para proyectos interdisciplinarios .  \n",
    "- **Ética en IA**:  \n",
    "  Módulos sobre sesgos algorítmicos y justicia social, clave en investigación cualitativa .  \n",
    "\n",
    "### **6. Recursos Locales (Colombia)**  \n",
    "- **Grupo GIPI (UdeA)**:  \n",
    "  Enfoque en ingeniería de procesos con IA. Colaboración potencial para proyectos aplicados .  \n",
    "- **Sergio Gutiérrez (UdeA)**:  \n",
    "  Investigador en redes neuronales para detección de intrusiones. Sus publicaciones ofrecen casos prácticos en seguridad .  \n",
    "\n",
    "---\n",
    "\n",
    "### **Recomendaciones para Implementación**  \n",
    "1. **En Docencia**: Usa ejemplos de *few-shot learning* (aprendizaje con pocos datos) para clases en entornos con recursos limitados .  \n",
    "2. **En Investigación**: Combina técnicas cuanti (análisis de RMSE en modelos) con cuali (interpretación de resultados con *attention maps* en CNNs) .  \n",
    "3. **Talleres Prácticos**:  \n",
    "   - \"Cómo entrenar un modelo de NLP con BERT\" (usando datasets en español).  \n",
    "   - \"Visualización de gradientes en redes convolucionales\" (para explicar matemáticas subyacentes) .  \n",
    "\n",
    "Estos referentes te permitirán actualizar tus cursos, integrar investigación aplicada y fomentar un enfoque crítico en el uso de IA. Para profundizar, explora los enlaces directos a los recursos citados."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
