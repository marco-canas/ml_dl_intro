{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65254a90",
   "metadata": {},
   "source": [
    "Aquí tienes la traducción al español del texto:\n",
    "\n",
    "---\n",
    "\n",
    "### El Perceptrón Multicapa y la Retropropagación  \n",
    "\n",
    "Un **Perceptrón Multicapa (MLP)** está compuesto por una capa de entrada, una o más capas de **Unidades Lineales Umbral (TLU)** llamadas **capas ocultas**, y una capa final de TLUs llamada **capa de salida** (ver Figura 10-7). Las capas cercanas a la entrada suelen llamarse **capas inferiores**, y las cercanas a las salidas, **capas superiores**.  \n",
    "\n",
    "**Figura 10-7.** Arquitectura de un perceptrón multicapa con dos entradas, una capa oculta de cuatro neuronas y tres neuronas de salida.  \n",
    "\n",
    "**NOTA**  \n",
    "El flujo de la señal solo va en una dirección (de las entradas a las salidas), por lo que esta arquitectura es un ejemplo de una **red neuronal de propagación hacia adelante (FNN)**.  \n",
    "\n",
    "Cuando una **red neuronal artificial (ANN)** contiene una pila profunda de capas ocultas, se llama **red neuronal profunda (DNN)**. El campo del **aprendizaje profundo** estudia las DNNs y, en general, se enfoca en modelos con pilas profundas de cálculos. Aun así, muchas personas hablan de aprendizaje profundo cada vez que se mencionan redes neuronales (incluso las superficiales).  \n",
    "\n",
    "Durante muchos años, los investigadores lucharon por encontrar una forma de entrenar MLPs sin éxito. A principios de los 60, varios investigadores discutieron la posibilidad de usar **descenso de gradiente** para entrenar redes neuronales, pero, como vimos en el Capítulo 4, esto requiere calcular los gradientes del error del modelo respecto a sus parámetros. En ese entonces no estaba claro cómo hacer esto eficientemente en un modelo tan complejo con tantos parámetros, especialmente con la capacidad computacional de la época.  \n",
    "\n",
    "En 1970, un investigador llamado **Seppo Linnainmaa** presentó en su tesis de maestría una técnica para calcular todos los gradientes de forma automática y eficiente. Este algoritmo se conoce hoy como **diferenciación automática en modo inverso** (o **autodiff inverso**). En solo dos pasadas por la red (una hacia adelante y otra hacia atrás), puede calcular los gradientes del error de la red neuronal respecto a cada parámetro del modelo. Es decir, determina cómo ajustar cada peso de conexión y cada sesgo para reducir el error. Estos gradientes se usan luego para realizar un paso de descenso de gradiente. Si se repite este proceso (calcular gradientes automáticamente y ajustar con descenso de gradiente), el error de la red neuronal disminuirá gradualmente hasta alcanzar un mínimo. Esta combinación de autodiff inverso y descenso de gradiente se llama **retropropagación** (o **backprop**).  \n",
    "\n",
    "**NOTA**  \n",
    "Existen varias técnicas de autodiff, cada una con pros y contras. El autodiff inverso es ideal cuando la función a diferenciar tiene muchas variables (pesos y sesgos) y pocas salidas (una pérdida).  \n",
    "\n",
    "La retropropagación puede aplicarse a todo tipo de grafos computacionales, no solo a redes neuronales: la tesis de Linnainmaa no trataba específicamente de redes neuronales, sino de un concepto más general. Pasaron varios años antes de que la retropropagación se usara ampliamente en redes neuronales.  \n",
    "\n",
    "En 1985, **David Rumelhart, Geoffrey Hinton y Ronald Williams** publicaron un artículo revolucionario que analizaba cómo la retropropagación permitía a las redes neuronales aprender representaciones internas útiles. Sus resultados fueron tan impactantes que la retropropagación se popularizó rápidamente. Hoy es, por mucho, la técnica de entrenamiento más usada en redes neuronales.  \n",
    "\n",
    "### Funcionamiento detallado de la retropropagación:  \n",
    "1. **Mini-lotes**: Procesa un mini-lote a la vez (ej. 32 instancias) y recorre el conjunto de entrenamiento múltiples veces. Cada pasada se llama **época**.  \n",
    "2. **Pasada hacia adelante**: El mini-lote entra por la capa de entrada, y se calcula la salida de cada neurona en las capas ocultas, preservando todos los resultados intermedios para la pasada hacia atrás.  \n",
    "3. **Cálculo del error**: Se mide el error de salida usando una **función de pérdida** que compara la salida deseada con la real.  \n",
    "4. **Pasada hacia atrás**: Usando la **regla de la cadena**, el algoritmo calcula cuánto contribuyó cada peso y sesgo al error, propagando el gradiente del error desde la salida hasta la entrada.  \n",
    "5. **Ajuste de pesos**: Finalmente, se realiza un paso de descenso de gradiente para ajustar los pesos y reducir el error.  \n",
    "\n",
    "**ADVERTENCIA**  \n",
    "Es crucial inicializar los pesos de las capas ocultas de forma **aleatoria**, de lo contrario el entrenamiento fallará. Si todos los pesos y sesgos empiezan en cero, todas las neuronas en una capa serán idénticas, y la retropropagación las ajustará igual, manteniendo la simetría. Esto haría que la red actúe como si tuviera solo una neurona por capa. La inicialización aleatoria rompe esta simetría y permite que la red aprenda diversidad.  \n",
    "\n",
    "En resumen, la retropropagación hace predicciones (pasada hacia adelante), mide el error, calcula las contribuciones al error por capa (pasada hacia atrás) y ajusta los pesos (descenso de gradiente).  \n",
    "\n",
    "### Activaciones no lineales:  \n",
    "Para que la retropropagación funcione, Rumelhart y sus colegas reemplazaron la **función escalón** por la **función logística** (σ(z) = 1 / (1 + exp(–z)), también llamada **sigmoide**). La función escalón tiene gradiente cero en todos lados (imposibilitando el descenso de gradiente), mientras que la sigmoide tiene un gradiente bien definido.  \n",
    "\n",
    "Otras funciones de activación populares:  \n",
    "- **Tangente hiperbólica (tanh)**: Similar a la sigmoide, pero con rango de –1 a 1, lo que acelera la convergencia.  \n",
    "- **Unidad Lineal Rectificada (ReLU)**: ReLU(z) = max(0, z). No es diferenciable en z = 0, pero en la práctica es eficiente y evita algunos problemas de gradiente.  \n",
    "\n",
    "**¿Por qué son necesarias las funciones de activación?**  \n",
    "Si solo se encadenan transformaciones lineales, el resultado sigue siendo lineal. Las activaciones no lineales permiten a las redes aproximar funciones complejas. Una DNN suficientemente grande con activaciones no lineales puede, en teoría, aproximar cualquier función continua.  \n",
    "\n",
    "**Figura 10-8.** Funciones de activación (izq.) y sus derivadas (der.).  \n",
    "\n",
    "¡Listo! Ahora conoces el origen de las redes neuronales, su arquitectura, cómo se calculan sus salidas y el algoritmo de retropropagación. Pero... ¿para qué sirven exactamente?  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377718d0",
   "metadata": {},
   "source": [
    "\n",
    "### Notas adicionales:  \n",
    "- Se mantuvieron términos técnicos como *backpropagation* (retropropagación) y *ReLU* por ser ampliamente usados en español.  \n",
    "- Se ajustaron ejemplos y fórmulas para claridad, conservando su significado original.  \n",
    "- El estilo es técnico pero accesible, dirigido a lectores con conocimientos básicos de machine learning.  \n",
    "\n",
    "¿Necesitas ajustes o más detalles en alguna sección?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad7b891",
   "metadata": {},
   "source": [
    "# Práctica de codificación  \n",
    "\n",
    "Aquí tienes una **práctica de codificación en Python** enfocada en redes neuronales multicapa (MLP) usando `TensorFlow/Keras`, que cubre los conceptos clave del texto traducido:  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e724ba2",
   "metadata": {},
   "source": [
    "\n",
    "### **Práctica: Implementación de un Perceptrón Multicapa (MLP) con Retropropagación**  \n",
    "**Objetivos:**  \n",
    "1. Crear un MLP secuencial para clasificación.  \n",
    "2. Entender el papel de las funciones de activación no lineales.  \n",
    "3. Visualizar el impacto de la inicialización de pesos.  \n",
    "4. Monitorizar el entrenamiento con retropropagación.  \n",
    "\n",
    "---  \n",
    "\n",
    "### **Paso 1: Configuración del Entorno**  \n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.initializers import RandomUniform, Zeros\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "```\n",
    "\n",
    "### **Paso 2: Generación de Datos**  \n",
    "Usamos el dataset `make_moons` para un problema de clasificación no lineal:  \n",
    "```python\n",
    "X, y = make_moons(n_samples=1000, noise=0.2, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis')\n",
    "plt.title(\"Dataset de Clasificación No Lineal\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### **Paso 3: Implementación del MLP con Keras**  \n",
    "#### **Modelo Secuencial con:**  \n",
    "- Capa oculta (4 neuronas, activación ReLU).  \n",
    "- Capa de salida (1 neurona, activación sigmoide).  \n",
    "- Función de pérdida: `binary_crossentropy` (para clasificación binaria).  \n",
    "- Optimizador: `Adam` (variante de descenso de gradiente).  \n",
    "\n",
    "```python\n",
    "model = Sequential([\n",
    "    Dense(4, activation='relu', input_shape=(2,), kernel_initializer=RandomUniform(minval=-0.5, maxval=0.5)),\n",
    "    Dense(1, activation='sigmoid', kernel_initializer=RandomUniform(minval=-0.5, maxval=0.5))\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "```\n",
    "\n",
    "### **Paso 4: Entrenamiento con Retropropagación**  \n",
    "```python\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test), verbose=1)\n",
    "```\n",
    "\n",
    "### **Paso 5: Visualización de Resultados**  \n",
    "#### **Curvas de Aprendizaje:**  \n",
    "```python\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Test Loss')\n",
    "plt.xlabel('Épocas')\n",
    "plt.ylabel('Pérdida')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "#### **Frontera de Decisión:**  \n",
    "```python\n",
    "def plot_decision_boundary(model, X, y):\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, levels=0.5, cmap='RdBu')\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', cmap='viridis')\n",
    "    plt.title(\"Frontera de Decisión del MLP\")\n",
    "    plt.show()\n",
    "\n",
    "plot_decision_boundary(model, X_test, y_test)\n",
    "```\n",
    "\n",
    "### **Paso 6: Experimentos Adicionales**  \n",
    "#### **1. Inicialización de Pesos a Cero (¡Advertencia!)**  \n",
    "```python\n",
    "model_zero_init = Sequential([\n",
    "    Dense(4, activation='relu', input_shape=(2,), kernel_initializer=Zeros()),\n",
    "    Dense(1, activation='sigmoid', kernel_initializer=Zeros())\n",
    "])\n",
    "model_zero_init.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "model_zero_init.fit(X_train, y_train, epochs=100, verbose=0)\n",
    "print(\"Precisión con pesos en cero:\", accuracy_score(y_test, model_zero_init.predict(X_test) > 0.5))\n",
    "```\n",
    "\n",
    "#### **2. Comparación de Funciones de Activación**  \n",
    "```python\n",
    "activations = ['relu', 'tanh', 'sigmoid']\n",
    "for activation in activations:\n",
    "    model_act = Sequential([\n",
    "        Dense(4, activation=activation, input_shape=(2,)),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model_act.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "    model_act.fit(X_train, y_train, epochs=100, verbose=0)\n",
    "    print(f\"Precisión con {activation}:\", accuracy_score(y_test, model_act.predict(X_test) > 0.5))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Conclusiones de la Práctica:**  \n",
    "- **Retropropagación:** El modelo ajusta automáticamente los pesos mediante `model.fit()`.  \n",
    "- **Funciones de Activación:** ReLU suele ser más eficiente que sigmoide/tanh en capas ocultas.  \n",
    "- **Inicialización:** Los pesos aleatorios rompen la simetría y permiten el aprendizaje.  \n",
    "- **Batch y Épocas:** El mini-batch (`batch_size=32`) acelera el entrenamiento.  \n",
    "\n",
    "**Salida Esperada:**  \n",
    "- Gráficos de pérdida decreciente.  \n",
    "- Frontera de decisión no lineal que separa las clases.  \n",
    "- Precisión > 90% con inicialización adecuada.  \n",
    "\n",
    "---  \n",
    "\n",
    "**¿Qué más te gustaría explorar?** Por ejemplo:  \n",
    "- Añadir más capas ocultas para crear una DNN.  \n",
    "- Regularización con Dropout.  \n",
    "- Optimización de hiperparámetros.  \n",
    "\n",
    "¡Espero que esta práctica te ayude a dominar los MLPs! 🚀"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
