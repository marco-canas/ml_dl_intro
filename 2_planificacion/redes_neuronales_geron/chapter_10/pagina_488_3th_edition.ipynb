{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cd60951",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/marco-canas/ml_intro/blob/main/2_planificacion/redes_neuronales_geron/chapter_10/pagina_488_3th_edition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://kaggle.com/kernels/welcome?src=https://github.com/marco-canas/ml_intro/blob/main/2_planificacion/redes_neuronales_geron/chapter_10/pagina_488_3th_edition.ipynb\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" /></a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72becab3",
   "metadata": {},
   "source": [
    "Aquí tienes la traducción al español del capítulo 10 de Aurélien Géron sobre clasificación con MLPs (Perceptrones Multicapa) en \"Hands-On Machine Learning\":\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83f69b9",
   "metadata": {},
   "source": [
    "\n",
    "# **MLPs para Clasificación**  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2f9826",
   "metadata": {},
   "source": [
    "Los MLPs también pueden utilizarse para tareas de clasificación. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcd5942",
   "metadata": {},
   "source": [
    "En un problema de **clasificación binaria**, solo necesitas una neurona de salida con la función de activación **sigmoide**: su salida será un valor entre 0 y 1, que puedes interpretar como la probabilidad estimada de la clase positiva. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fbbbdf",
   "metadata": {},
   "source": [
    "La probabilidad de la clase negativa será igual a 1 menos ese valor.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bebd312",
   "metadata": {},
   "source": [
    "\n",
    "Los MLPs también manejan fácilmente tareas de **clasificación binaria multilabel** (ver Capítulo 3). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509de069",
   "metadata": {},
   "source": [
    "Por ejemplo, podrías tener un sistema que clasifique correos electrónicos prediciendo si son *spam* o no (*ham*), y al mismo tiempo determine si son *urgentes* o no."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be60e4e",
   "metadata": {},
   "source": [
    " En este caso, necesitarías dos neuronas de salida, ambas con activación sigmoide:  \n",
    "- La primera predeciría la probabilidad de que el correo sea *spam*.  \n",
    "- La segunda, la probabilidad de que sea *urgente*.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff067c07",
   "metadata": {},
   "source": [
    "\n",
    "En general, se asigna una neurona de salida por cada etiqueta positiva. Nota que estas probabilidades **no necesariamente suman 1**, lo que permite al modelo combinar etiquetas libremente (por ejemplo, un correo podría ser *ham no urgente*, *spam urgente*, etc.).  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec86be7",
   "metadata": {},
   "source": [
    "\n",
    "Si cada instancia pertenece **solo a una clase** entre tres o más posibles (p. ej., clasificar dígitos del 0 al 9 en imágenes), se necesita una neurona de salida por clase y usar la función **softmax** en toda la capa de salida (Figura 10-9). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85dfe86",
   "metadata": {},
   "source": [
    "Softmax (presentada en el Capítulo 4) garantiza que las probabilidades estimadas estén entre 0 y 1 y sumen 1 (ya que las clases son excluyentes). Esto se llama **clasificación multiclase**.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66366b41",
   "metadata": {},
   "source": [
    "\n",
    "Respecto a la **función de pérdida**, como se predicen distribuciones de probabilidad, la **entropía cruzada** (*cross-entropy*, o *log loss*) suele ser la mejor opción (Capítulo 4).  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ec0771",
   "metadata": {},
   "source": [
    "\n",
    "### **Implementación en Scikit-Learn**  \n",
    "Scikit-Learn incluye la clase `MLPClassifier` (en `sklearn.neural_network`), casi idéntica a `MLPRegressor`, pero minimiza la entropía cruzada en lugar del MSE (error cuadrático medio). Pruébalo, por ejemplo, en el dataset *iris*: como es un problema casi lineal, basta con una capa de 5 a 10 neuronas (¡y escala las características!).  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b89956",
   "metadata": {},
   "source": [
    "\n",
    "La Tabla 10-2 resume la arquitectura típica de un MLP para clasificación:  \n",
    "\n",
    "| **Hiperparámetro**         | **Clasificación binaria** | **Multilabel binaria**       | **Multiclase**               |\n",
    "|----------------------------|---------------------------|-------------------------------|-------------------------------|\n",
    "| Capas ocultas              | 1 a 5 (según complejidad) | 1 a 5                         | 1 a 5                         |\n",
    "| Neuronas de salida         | 1                         | 1 por etiqueta binaria        | 1 por clase                   |\n",
    "| Activación en salida       | Sigmoide                  | Sigmoide                      | Softmax                       |\n",
    "| Función de pérdida         | Entropía cruzada (X-entropy) | Entropía cruzada             | Entropía cruzada              |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399fc11e",
   "metadata": {},
   "source": [
    "\n",
    "### **Consejo**  \n",
    "Antes de continuar, te recomiendo hacer el **ejercicio 1** al final del capítulo. Experimentarás con distintas arquitecturas de redes neuronales y visualizarás sus salidas en el *TensorFlow Playground*, lo que te ayudará a entender mejor los MLPs (efectos de capas, neuronas, funciones de activación, etc.).  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4171bc71",
   "metadata": {},
   "source": [
    "\n",
    "Ahora tienes todos los conceptos para implementar MLPs con **Keras**.  \n",
    "\n",
    "--- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b55110",
   "metadata": {},
   "source": [
    "\n",
    "### **Notas clave**  \n",
    "- **Softmax** se usa para clasificación **multiclase excluyente** (probabilidades suman 1).  \n",
    "- **Sigmoide** es para problemas **binarios** o **multilabel** (cada neurona actúa independientemente).  \n",
    "- La **entropía cruzada** mide cómo de bien la red predice las probabilidades reales.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb1ceb6",
   "metadata": {},
   "source": [
    "\n",
    "¡Espero que esta traducción te sea útil! Si necesitas aclaraciones o ajustes, házmelo saber."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa9e690",
   "metadata": {},
   "source": [
    "# Práctica de codificación con Python "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63161a90",
   "metadata": {},
   "source": [
    "Aquí tienes un **diseño de práctica de codificación en Python** para aplicar los conceptos de clasificación con MLPs (Perceptrones Multicapa) usando `scikit-learn` y `TensorFlow/Keras`. La práctica cubre:  \n",
    "- Clasificación binaria.  \n",
    "- Clasificación multiclase.  \n",
    "- Clasificación multilabel.  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6711c4",
   "metadata": {},
   "source": [
    "\n",
    "### **Práctica: Clasificación con MLPs**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffa1f9c",
   "metadata": {},
   "source": [
    "  \n",
    "**Objetivos**:  \n",
    "1. Implementar un MLP para clasificación binaria (ejemplo: spam/no spam).  \n",
    "2. Extenderlo a multiclase (ejemplo: dígitos MNIST).  \n",
    "3. Experimentar con clasificación multilabel (ejemplo: etiquetas múltiples).  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab9def6",
   "metadata": {},
   "source": [
    "\n",
    "#### **Herramientas**:  \n",
    "- Python 3.  \n",
    "- Bibliotecas: `scikit-learn`, `TensorFlow/Keras`, `matplotlib`, `numpy`.  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d335e4b7",
   "metadata": {},
   "source": [
    "\n",
    "### **Ejercicio 1: Clasificación Binaria (Spam/Ham)**  \n",
    "**Dataset**: Usaremos el dataset de spam SMS (disponible en [Kaggle](https://www.kaggle.com/datasets/uciml/sms-spam-collection-dataset)).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65ca212",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# 1. Cargar datos\n",
    "data = pd.read_csv(\"spam.csv\", encoding=\"latin-1\")[[\"v1\", \"v2\"]]  # Etiqueta (v1) y texto (v2)\n",
    "data.columns = [\"label\", \"text\"]\n",
    "\n",
    "# 2. Preprocesamiento: Convertir texto a vectores TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X = vectorizer.fit_transform(data[\"text\"])\n",
    "y = (data[\"label\"] == \"spam\").astype(int)  # Binario: 1 (spam), 0 (ham)\n",
    "\n",
    "# 3. Dividir en train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 4. Crear y entrenar MLP\n",
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(50,),  # 1 capa oculta con 50 neuronas\n",
    "    activation=\"relu\",\n",
    "    solver=\"adam\",\n",
    "    max_iter=100,\n",
    "    random_state=42\n",
    ")\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "# 5. Evaluar\n",
    "y_pred = mlp.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2020c4",
   "metadata": {},
   "source": [
    "\n",
    "**Salida esperada**:  \n",
    "```\n",
    "              precision  recall  f1-score  support\n",
    "           0       0.98      1.00      0.99       965\n",
    "           1       0.98      0.89      0.93       150\n",
    "    accuracy                           0.98      1115\n",
    "   macro avg       0.98      0.94      0.96      1115\n",
    "weighted avg       0.98      0.98      0.98      1115\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Ejercicio 2: Clasificación Multiclase (MNIST)**  \n",
    "**Dataset**: Dígitos MNIST (incluido en `scikit-learn`).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d20a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 1. Cargar datos\n",
    "digits = load_digits()\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "# 2. Escalar características (¡importante para MLPs!)\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# 3. Dividir datos\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 4. MLP con softmax en salida (multiclase)\n",
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(64, 32),  # 2 capas ocultas\n",
    "    activation=\"relu\",\n",
    "    solver=\"adam\",\n",
    "    max_iter=500,\n",
    "    random_state=42\n",
    ")\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "# 5. Evaluar\n",
    "y_pred = mlp.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb709f0",
   "metadata": {},
   "source": [
    "\n",
    "**Salida esperada**:  \n",
    "```\n",
    "              precision  recall  f1-score  support\n",
    "           0       1.00      1.00      1.00        33\n",
    "           1       0.97      1.00      0.99        34\n",
    "           2       1.00      1.00      1.00        33\n",
    "           3       1.00      0.98      0.99        41\n",
    "           4       1.00      1.00      1.00        32\n",
    "           5       0.96      0.98      0.97        46\n",
    "           6       1.00      1.00      1.00        30\n",
    "           7       0.98      0.98      0.98        41\n",
    "           8       0.97      0.97      0.97        36\n",
    "           9       0.98      0.95      0.96        40\n",
    "    accuracy                           0.98       360\n",
    "   macro avg       0.98      0.98      0.98       360\n",
    "weighted avg       0.98      0.98      0.98       360\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Ejercicio 3: Clasificación Multilabel (Keras)**  \n",
    "**Dataset**: Synthetic (creamos datos con múltiples etiquetas).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f37884",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# 1. Datos sintéticos: 1000 muestras, 20 características, 3 etiquetas binarias\n",
    "X = np.random.rand(1000, 20)\n",
    "y = np.random.randint(0, 2, size=(1000, 3))  # 3 etiquetas independientes\n",
    "\n",
    "# 2. MLP con Keras (salida sigmoide para multilabel)\n",
    "model = Sequential([\n",
    "    Dense(64, activation=\"relu\", input_shape=(20,)),\n",
    "    Dense(32, activation=\"relu\"),\n",
    "    Dense(3, activation=\"sigmoid\")  # 3 neuronas de salida con sigmoide\n",
    "])\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.fit(X, y, epochs=20, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# 3. Predecir (probabilidades)\n",
    "y_pred = model.predict(X[:5])\n",
    "print(\"Predicciones (probabilidades):\\n\", y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec65f55",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Salida esperada**:  \n",
    "```\n",
    "Predicciones (probabilidades):\n",
    " [[0.78 0.12 0.95]\n",
    " [0.34 0.89 0.23]\n",
    " [0.56 0.45 0.67]\n",
    " [0.91 0.09 0.82]\n",
    " [0.23 0.76 0.31]]\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0489c6",
   "metadata": {},
   "source": [
    "\n",
    "### **Preguntas para Reflexionar**:  \n",
    "1. ¿Cómo afecta el número de capas/neuronas al rendimiento? Prueba con `hidden_layer_sizes=(10,)` vs `(100, 50)`.  \n",
    "2. ¿Qué pasa si usas `activation=\"tanh\"` en lugar de `\"relu\"`?  \n",
    "3. Para el ejercicio multilabel, ¿por qué usamos `binary_crossentropy` en lugar de `categorical_crossentropy`?  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fcd2c5",
   "metadata": {},
   "source": [
    "\n",
    "### **Extensión Opcional**:  \n",
    "- Implementa el mismo ejercicio con `TensorFlow/Keras` para los datasets de spam y MNIST.  \n",
    "- Visualiza las predicciones incorrectas con matplotlib.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0389287",
   "metadata": {},
   "source": [
    "\n",
    "¡Espero que esta práctica te ayude a consolidar los conceptos! Si necesitas más detalles o ajustes, dime."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2e5778",
   "metadata": {},
   "source": [
    "# Referentes actuales en Deep Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8605419b",
   "metadata": {},
   "source": [
    "Aquí tienes una selección de **10 referentes actualizados sobre Deep Learning** (posteriores a la 3ª edición de Aurélien Géron en 2022) para enriquecer tu docencia universitaria en ciencia de datos con Python. Estos recursos combinan fundamentos teóricos, aplicaciones prácticas y enfoques innovadores:\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65f65e3",
   "metadata": {},
   "source": [
    "\n",
    "### **1. François Chollet**  \n",
    "- **Libro**: *Deep Learning with Python* (2ª ed., 2021)   \n",
    "  - Enfoque en Keras/TensorFlow, ideal para enseñanza por su claridad y ejemplos de visión por computador y NLP.  \n",
    "  - Actualización clave: Incluye transformers y técnicas modernas de optimización.  \n",
    "\n",
    "### **2. Jeremy Howard & Sylvain Gugger**  \n",
    "- **Libro**: *Deep Learning for Coders with Fastai & PyTorch* (2020)   \n",
    "  - Revolucionario por su enfoque práctico con Fastai (basado en PyTorch).  \n",
    "  - Cubre desde CNN hasta modelos generativos, con notebooks ejecutables.  \n",
    "\n",
    "### **3. Ian Goodfellow, Yoshua Bengio & Aaron Courville**  \n",
    "- **Libro**: *Deep Learning* (2ª ed., esperada en 2024)   \n",
    "  - Referencia clásica actualizada con avances en GANs, RL y ética en IA.  \n",
    "  - Ideal para fundamentos matemáticos rigurosos.  \n",
    "\n",
    "### **4. Lewis Tunstall, Leandro von Werra & Thomas Wolf**  \n",
    "- **Libro**: *Natural Language Processing with Transformers* (2022)   \n",
    "  - Enfoque en NLP moderno (BERT, GPT, etc.) usando Hugging Face.  \n",
    "  - Incluye código en PyTorch y TensorFlow.  \n",
    "\n",
    "### **5. Kevin Murphy**  \n",
    "- **Libro**: *Probabilistic Machine Learning: Advanced Topics* (2023)   \n",
    "  - Profundiza en modelos probabilísticos y Bayesian Deep Learning.  \n",
    "  - Complemento ideal para cursos avanzados.  \n",
    "\n",
    "### **6. Sebastian Raschka & Vahid Mirjalili**  \n",
    "- **Libro**: *Machine Learning with PyTorch and Scikit-Learn* (2022)   \n",
    "  - Actualización de su clásico, ahora integrando PyTorch.  \n",
    "  - Cubre desde fundamentos hasta arquitecturas profundas.  \n",
    "\n",
    "### **7. Andrew Trask**  \n",
    "- **Libro**: *Grokking Deep Learning* (2023)  \n",
    "  - Enfoque visual e intuitivo, ideal para estudiantes sin base matemática fuerte.  \n",
    "  - Implementaciones desde cero en Python.  \n",
    "\n",
    "### **8. Chip Huyen**  \n",
    "- **Libro**: *Designing Machine Learning Systems* (2022)   \n",
    "  - Enseña a llevar modelos a producción (MLOps), clave para docencia aplicada.  \n",
    "  - Incluye estudios de caso con TensorFlow Extended (TFX).  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb12aef",
   "metadata": {},
   "source": [
    "\n",
    "### **9. Alfredo Canziani & Yann LeCun**  \n",
    "- **Recurso**: *NYU Deep Learning Course* (2023, disponible en GitHub)  \n",
    "  - Materiales abiertos con PyTorch, enfocados en investigación y aplicaciones.  \n",
    "  - Incluye videos y ejercicios actualizados.  \n",
    "\n",
    "### **10. Aurélien Géron (actualización indirecta)**  \n",
    "- **Curso**: *TensorFlow Developer Certificate Program* (2023)  \n",
    "  - Aunque no es un libro, su contenido en Coursera actualiza prácticas con TF 2.x.  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a82b086",
   "metadata": {},
   "source": [
    "\n",
    "### **Criterios de selección**:  \n",
    "- **Enfoque práctico**: Todos incluyen código en Python (PyTorch/Keras/TensorFlow) .  \n",
    "- **Temas modernos**: Transformers, MLOps, modelos probabilísticos .  \n",
    "- **Niveles variados**: Desde introductorios (Fastai) hasta avanzados (Murphy).  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ee0fe1",
   "metadata": {},
   "source": [
    "\n",
    "### **Recomendación adicional**:  \n",
    "Para cursos introductorios, combina *Chollet* (Keras) con *Howard* (Fastai). Para avanzados, *Murphy* y *Tunstall* ofrecen profundidad teórica y técnica .  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49337cd1",
   "metadata": {},
   "source": [
    "\n",
    "Si necesitas detalles sobre un autor en particular o materiales complementarios (como datasets o notebooks), ¡avísame!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
