{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cd60951",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/marco-canas/ml_intro/blob/main/2_planificacion/redes_neuronales_geron/chapter_10/pagina_488_3th_edition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://kaggle.com/kernels/welcome?src=https://github.com/marco-canas/ml_intro/blob/main/2_planificacion/redes_neuronales_geron/chapter_10/pagina_488_3th_edition.ipynb\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" /></a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72becab3",
   "metadata": {},
   "source": [
    "Aquí tienes la traducción al español del capítulo 10 de Aurélien Géron sobre clasificación con MLPs (Perceptrones Multicapa) en \"Hands-On Machine Learning\":\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83f69b9",
   "metadata": {},
   "source": [
    "\n",
    "# **MLPs para Clasificación**  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2f9826",
   "metadata": {},
   "source": [
    "Los MLPs también pueden utilizarse para tareas de clasificación. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcd5942",
   "metadata": {},
   "source": [
    "En un problema de **clasificación binaria**, solo necesitas una neurona de salida con la función de activación **sigmoide**: su salida será un valor entre 0 y 1, que puedes interpretar como la probabilidad estimada de la clase positiva. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2630ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Definir la función sigmoide\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "x = np.linspace(start = -10, stop = 10, num = 200) # partición regular del intervalo [-10, 10]\n",
    "y = sigmoid(x)\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(x, y, color = 'red') # gráfico de líneas \n",
    "plt.axvline(x = 0) # eje horizontal \n",
    "plt.axhline(y = 0) # eje vertical \n",
    "plt.title(\"Función Sigmoide\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"sigmoid(x)\")\n",
    "plt.xticks(np.arange(-10, 10))\n",
    "plt.yticks(np.arange(0,1+ 0.1, 0.1))\n",
    "plt.grid(True)\n",
    "plt.savefig(r'C:\\Users\\marco\\Downloads\\sigmoid_plot.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fbbbdf",
   "metadata": {},
   "source": [
    "La probabilidad de la clase negativa será igual a 1 menos ese valor.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d099122",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5), sharex=True) # crea una figura o lienzo con dos planos \n",
    "\n",
    "# Probabilidad clase positiva (sigmoide)\n",
    "ax1.plot(x, y, color='red')\n",
    "ax1.set_title('Probabilidad clase positiva (sigmoide)')\n",
    "ax1.set_ylabel('P(clase positiva)')\n",
    "ax1.grid(True)\n",
    "\n",
    "# Probabilidad clase negativa (1 - sigmoide)\n",
    "ax2.plot(x, 1 - y, color='blue')\n",
    "ax2.set_title('Probabilidad clase negativa (1 - sigmoide)')\n",
    "ax2.set_xlabel('x')\n",
    "ax2.set_ylabel('P(clase negativa)')\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bebd312",
   "metadata": {},
   "source": [
    "\n",
    "Los MLPs también manejan fácilmente tareas de **clasificación binaria multilabel** (ver Capítulo 3). Decimos que la clasificación es multilabel o multetiqueta cuando las neuronas de salida son independientes.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509de069",
   "metadata": {},
   "source": [
    "Por ejemplo, podrías tener un sistema que clasifique correos electrónicos prediciendo si son *spam* o no (*ham*), y al mismo tiempo determine si son *urgentes* o no."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a188e8d4",
   "metadata": {},
   "source": [
    "<img src = 'spam_ham_urgent_no_urgent.png' width = 500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be60e4e",
   "metadata": {},
   "source": [
    " En este caso, necesitarías dos neuronas de salida, ambas con activación sigmoide:  \n",
    "- La primera predeciría la probabilidad de que el correo sea *spam*.  \n",
    "- La segunda, la probabilidad de que sea *urgente*.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff067c07",
   "metadata": {},
   "source": [
    "\n",
    "En general, se asigna una neurona de salida por cada etiqueta positiva. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfa8ae1",
   "metadata": {},
   "source": [
    "Nota que estas probabilidades **no necesariamente suman 1**, lo que permite al modelo combinar etiquetas libremente (por ejemplo, un correo podría ser *ham no urgente*, *spam urgente*, etc.).  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1146c59",
   "metadata": {},
   "source": [
    "# Clasificación multiplclase\n",
    "\n",
    "Se da cuendo las salidas son dependientes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec86be7",
   "metadata": {},
   "source": [
    "\n",
    "Si cada instancia pertenece **solo a una clase** entre tres o más posibles (p. ej., clasificar dígitos del 0 al 9 en imágenes), se necesita una neurona de salida por clase y usar la función **softmax** en toda la capa de salida (Figura 10-9). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6eab7b1",
   "metadata": {},
   "source": [
    "$$ softmax(x) = \\frac{ex}{\\sum ex_{i}} = \\frac{e^{x - \\max(x)}}{\\sum e^{x - \\max(x)}} = \\frac{1}{\\sum e^{x - \\max(x)}} e^{x - \\max(x)} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37c8b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "x = np.array([2, 3, 1])\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "y_softmax = softmax(x)\n",
    "\n",
    "plt.scatter(x, y_softmax)\n",
    "plt.title(\"Función Softmax\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"softmax(x)\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589d32d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(y_softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85dfe86",
   "metadata": {},
   "source": [
    "Softmax (presentada en el Capítulo 4) garantiza que las probabilidades estimadas estén entre 0 y 1 y sumen 1 (ya que las clases son excluyentes). Esto se llama **clasificación multiclase**.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86378cd",
   "metadata": {},
   "source": [
    "# Sobre la función de pérdida "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66366b41",
   "metadata": {},
   "source": [
    "\n",
    "Respecto a la **función de pérdida**, como se predicen distribuciones de probabilidad, la **entropía cruzada** (*cross-entropy*, o *log loss*) suele ser la mejor opción (Capítulo 4).  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a21958d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Entropía cruzada binaria: -[y_true*log(y_pred) + (1-y_true)*log(1-y_pred)]\n",
    "# Mostramos para y_true=1 (clase positiva) y y_true=0 (clase negativa)\n",
    "y_pred = np.linspace(1e-6, 1-1e-6, 200) # una partición regular de (0,1] de 200 puntos. El 1e-6 es para evitar el log(0)\n",
    "loss_pos = -np.log(y_pred)           # y_true = 1\n",
    "loss_neg = -np.log(1 - y_pred)       # y_true = 0\n",
    "\n",
    "plt.plot(y_pred, loss_pos, label=\"y_true = 1\")\n",
    "plt.plot(y_pred, loss_neg, label=\"y_true = 0\")\n",
    "plt.title(\"Entropía cruzada binaria (log loss)\")\n",
    "plt.xlabel(\"Predicción de probabilidad (y_pred)\")\n",
    "plt.ylabel(\"Pérdida (loss)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15e0ad2",
   "metadata": {
    "vscode": {
     "languageId": "yaml"
    }
   },
   "source": [
    "\n",
    "* \"El gráfico anterior muestra la función de pérdida de entropía cruzada binaria (log loss) para clasificación binaria. \"\n",
    "* \"Las dos curvas representan la pérdida para la clase positiva (y_true=1) y la clase negativa (y_true=0) en función de la probabilidad predicha (y_pred). \"\n",
    "* \"Interpretación:\\n\"\n",
    "    * \"- Cuando la predicción es correcta (y_pred cerca de 1 para y_true=1, o cerca de 0 para y_true=0), la pérdida es baja.\\n\"\n",
    "    * \"- Cuando la predicción es incorrecta (y_pred cerca de 0 para y_true=1, o cerca de 1 para y_true=0), la pérdida crece rápidamente.\\n\"\n",
    "    * \"Esto penaliza fuertemente las predicciones erróneas y motiva al modelo a asignar alta probabilidad a la clase correcta.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ec0771",
   "metadata": {},
   "source": [
    "\n",
    "# **Implementación en Scikit-Learn**  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1970d21",
   "metadata": {},
   "source": [
    "Scikit-Learn incluye la clase `MLPClassifier` (en `sklearn.neural_network`), casi idéntica a `MLPRegressor`, pero minimiza la entropía cruzada en lugar del MSE (error cuadrático medio). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530e1555",
   "metadata": {},
   "source": [
    "Pruébalo, por ejemplo, en el dataset *iris*: como es un problema casi lineal, basta con una capa de 5 a 10 neuronas (¡y escala las características!).  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b89956",
   "metadata": {},
   "source": [
    "\n",
    "La Tabla 10-2 resume la arquitectura típica de un MLP para clasificación:  \n",
    "\n",
    "| **Hiperparámetro**         | **Clasificación binaria** | **Multilabel binaria**       | **Multiclase**               |\n",
    "|----------------------------|---------------------------|-------------------------------|-------------------------------|\n",
    "| Capas ocultas              | 1 a 5 (según complejidad) | 1 a 5                         | 1 a 5                         |\n",
    "| Neuronas de salida         | 1                         | 1 por etiqueta binaria        | 1 por clase                   |\n",
    "| Activación en salida       | Sigmoide                  | Sigmoide                      | Softmax                       |\n",
    "| Función de pérdida         | Entropía cruzada (X-entropy) | Entropía cruzada             | Entropía cruzada              |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399fc11e",
   "metadata": {},
   "source": [
    "\n",
    "# **Consejo**  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaaed58c",
   "metadata": {},
   "source": [
    "Antes de continuar, te recomiendo hacer el **ejercicio 1** al final del capítulo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c85718",
   "metadata": {},
   "source": [
    "Experimentarás con distintas arquitecturas de redes neuronales y visualizarás sus salidas en el [*TensorFlow Playground*](https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.00405&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false), lo que te ayudará a entender mejor los MLPs (efectos de capas, neuronas, funciones de activación, etc.).  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4171bc71",
   "metadata": {},
   "source": [
    "\n",
    "Ahora tienes todos los conceptos para implementar MLPs con **Keras**.  \n",
    "\n",
    "--- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b55110",
   "metadata": {},
   "source": [
    "\n",
    "### **Notas clave**  \n",
    "- **Softmax** se usa para clasificación **multiclase excluyente** (probabilidades suman 1).  \n",
    "- **Sigmoide** es para problemas **binarios** o **multilabel** (cada neurona actúa independientemente).  \n",
    "- La **entropía cruzada** mide cómo de bien la red predice las probabilidades reales.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63d05e6",
   "metadata": {},
   "source": [
    "# Ejemplo con el dataset de flores de iris Clasificación Multiclase "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722ef82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris # obtener los datos \n",
    "from sklearn.model_selection import train_test_split # \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b8c3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ejemplo con el dataset de flores de iris  \n",
    "import pandas as pd \n",
    "# Cargar el dataset iris\n",
    "iris = load_iris()\n",
    "df_iris = pd.DataFrame(iris.data, columns = iris.feature_names)\n",
    "df_iris['target'] = iris.target\n",
    "df_iris.head() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06dd8d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris.data # Obtener predictores \n",
    "y = iris.target # Obtener las etiquetas \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8dfb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Escalar características con la técnica de estandarización \n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4899481",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Dividir en train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, \\\n",
    "    test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240e61d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%time \n",
    "# Crear y entrenar el MLP\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(10,), activation=\"relu\", \\\n",
    "    max_iter=300, random_state=42)\n",
    "mlp.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e39c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluar\n",
    "y_pred = mlp.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, target_names=iris.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0705d970",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gráfica de 30 instancias de testeo, el 20% de 150 instancias u observaciones \n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visualización de la predicción vs real para el set de test\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(y_test, 'o-', label='Real', color='tab:blue')\n",
    "plt.plot(y_pred, 's--', label='Predicho', color='tab:orange')\n",
    "plt.title('Clasificación multiclase: valores reales vs predichos (Iris)')\n",
    "plt.xlabel('Índice de muestra')\n",
    "plt.ylabel('Clase')\n",
    "plt.yticks(np.unique(y_test), iris.target_names)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(r\"C:\\Users\\marco\\Downloads\\clasificacion_multiclase.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa9e690",
   "metadata": {},
   "source": [
    "# Práctica de codificación con Python "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63161a90",
   "metadata": {},
   "source": [
    "Aquí tienes un **diseño de práctica de codificación en Python** para aplicar los conceptos de clasificación con MLPs (Perceptrones Multicapa) usando `scikit-learn` y `TensorFlow/Keras`. La práctica cubre:  \n",
    "- Clasificación binaria.  \n",
    "- Clasificación multiclase.  \n",
    "- Clasificación multilabel.  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6711c4",
   "metadata": {},
   "source": [
    "\n",
    "### **Práctica: Clasificación con MLPs**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffa1f9c",
   "metadata": {},
   "source": [
    "  \n",
    "**Objetivos**:  \n",
    "1. Implementar un MLP para clasificación binaria (ejemplo: spam/no spam).  \n",
    "2. Extenderlo a multiclase (ejemplo: dígitos MNIST).  \n",
    "3. Experimentar con clasificación multilabel (ejemplo: etiquetas múltiples).  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab9def6",
   "metadata": {},
   "source": [
    "\n",
    "#### **Herramientas**:  \n",
    "- Python 3.  \n",
    "- Bibliotecas: `scikit-learn`, `TensorFlow/Keras`, `matplotlib`, `numpy`.  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d335e4b7",
   "metadata": {},
   "source": [
    "\n",
    "### **Ejercicio 1: Clasificación Binaria (Spam/Ham)**  \n",
    "**Dataset**: Usaremos el dataset de spam SMS (disponible en [Kaggle](https://www.kaggle.com/datasets/uciml/sms-spam-collection-dataset)).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65ca212",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e1dd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Cargar datos\n",
    "data = pd.read_csv(r\"C:\\Users\\marco\\Documentos\\investigacion\\ml_intro\\2_planificacion\\redes_neuronales_geron\\geron\\10_chapter\\datasets\\spam.csv\",\\\n",
    "    encoding=\"latin-1\")[[\"v1\", \"v2\"]]  # Etiqueta (v1) y texto (v2)\n",
    "data.columns = [\"label\", \"text\"]\n",
    "data.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba468e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2. Preprocesamiento: Convertir texto a vectores TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X = vectorizer.fit_transform(data[\"text\"])\n",
    "y = (data[\"label\"] == \"spam\").astype(int)  # Binario: 1 (spam), 0 (ham)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1ab957",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3. Dividir en train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52defe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4. Crear y entrenar MLP\n",
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(50,),  # 1 capa oculta con 50 neuronas\n",
    "    activation=\"relu\",\n",
    "    solver=\"adam\",\n",
    "    max_iter=100,\n",
    "    random_state=42\n",
    ")\n",
    "mlp.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39da3e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 5. Evaluar\n",
    "y_pred = mlp.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2020c4",
   "metadata": {},
   "source": [
    "\n",
    "**Salida esperada**:  \n",
    "```\n",
    "              precision  recall  f1-score  support\n",
    "           0       0.98      1.00      0.99       965\n",
    "           1       0.98      0.89      0.93       150\n",
    "    accuracy                           0.98      1115\n",
    "   macro avg       0.98      0.94      0.96      1115\n",
    "weighted avg       0.98      0.98      0.98      1115\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Ejercicio 2: Clasificación Multiclase (MNIST)**  \n",
    "**Dataset**: Dígitos MNIST (incluido en `scikit-learn`).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d20a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07efc8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Cargar datos\n",
    "digits = load_digits()\n",
    "X = digits.data\n",
    "y = digits.target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19ce6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2. Escalar características (¡importante para MLPs!)\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c26809",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3. Dividir datos\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \\\n",
    "    test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76315258",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4. MLP con softmax en salida (multiclase)\n",
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(64, 32),  # 2 capas ocultas\n",
    "    activation=\"relu\",\n",
    "    solver=\"adam\",\n",
    "    max_iter=500,\n",
    "    random_state=42\n",
    ")\n",
    "mlp.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d17c7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 5. Evaluar\n",
    "y_pred = mlp.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb709f0",
   "metadata": {},
   "source": [
    "\n",
    "**Salida esperada**:  \n",
    "```\n",
    "              precision  recall  f1-score  support\n",
    "           0       1.00      1.00      1.00        33\n",
    "           1       0.97      1.00      0.99        34\n",
    "           2       1.00      1.00      1.00        33\n",
    "           3       1.00      0.98      0.99        41\n",
    "           4       1.00      1.00      1.00        32\n",
    "           5       0.96      0.98      0.97        46\n",
    "           6       1.00      1.00      1.00        30\n",
    "           7       0.98      0.98      0.98        41\n",
    "           8       0.97      0.97      0.97        36\n",
    "           9       0.98      0.95      0.96        40\n",
    "    accuracy                           0.98       360\n",
    "   macro avg       0.98      0.98      0.98       360\n",
    "weighted avg       0.98      0.98      0.98       360\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Ejercicio 3: Clasificación Multilabel (Keras)**  \n",
    "**Dataset**: Synthetic (creamos datos con múltiples etiquetas).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f37884",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f9ab09",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Datos sintéticos: 1000 muestras, 20 características, 3 etiquetas binarias\n",
    "X = np.random.rand(1000, 20)\n",
    "y = np.random.randint(0, 2, size=(1000, 3))  # 3 etiquetas independientes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60c39dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%time \n",
    "# 2. MLP con Keras (salida sigmoide para multilabel)\n",
    "model = Sequential([\n",
    "    Dense(64, activation=\"relu\", input_shape=(20,)),\n",
    "    Dense(32, activation=\"relu\"),\n",
    "    Dense(3, activation=\"sigmoid\")  # 3 neuronas de salida con sigmoide\n",
    "])\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.fit(X, y, epochs=20, batch_size=32, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee39f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3. Predecir (probabilidades)\n",
    "y_pred = model.predict(X[:5])\n",
    "print(\"Predicciones (probabilidades):\\n\", y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec65f55",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Salida esperada**:  \n",
    "```\n",
    "Predicciones (probabilidades):\n",
    " [[0.78 0.12 0.95]\n",
    " [0.34 0.89 0.23]\n",
    " [0.56 0.45 0.67]\n",
    " [0.91 0.09 0.82]\n",
    " [0.23 0.76 0.31]]\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0489c6",
   "metadata": {},
   "source": [
    "\n",
    "### **Preguntas para Reflexionar**:  \n",
    "1. ¿Cómo afecta el número de capas/neuronas al rendimiento? Prueba con `hidden_layer_sizes=(10,)` vs `(100, 50)`.  \n",
    "2. ¿Qué pasa si usas `activation=\"tanh\"` en lugar de `\"relu\"`?  \n",
    "3. Para el ejercicio multilabel, ¿por qué usamos `binary_crossentropy` en lugar de `categorical_crossentropy`?  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fcd2c5",
   "metadata": {},
   "source": [
    "\n",
    "### **Extensión Opcional**:  \n",
    "- Implementa el mismo ejercicio con `TensorFlow/Keras` para los datasets de spam y MNIST.  \n",
    "- Visualiza las predicciones incorrectas con matplotlib.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0389287",
   "metadata": {},
   "source": [
    "\n",
    "¡Espero que esta práctica te ayude a consolidar los conceptos! Si necesitas más detalles o ajustes, dime."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2e5778",
   "metadata": {},
   "source": [
    "# Referentes actuales en Deep Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8605419b",
   "metadata": {},
   "source": [
    "Aquí tienes una selección de **10 referentes actualizados sobre Deep Learning** (posteriores a la 3ª edición de Aurélien Géron en 2022) para enriquecer tu docencia universitaria en ciencia de datos con Python. Estos recursos combinan fundamentos teóricos, aplicaciones prácticas y enfoques innovadores:\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65f65e3",
   "metadata": {},
   "source": [
    "\n",
    "### **1. François Chollet**  \n",
    "- **Libro**: *Deep Learning with Python* (2ª ed., 2021)   \n",
    "  - Enfoque en Keras/TensorFlow, ideal para enseñanza por su claridad y ejemplos de visión por computador y NLP.  \n",
    "  - Actualización clave: Incluye transformers y técnicas modernas de optimización.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dad8891",
   "metadata": {},
   "source": [
    "\n",
    "### **2. Jeremy Howard & Sylvain Gugger**  \n",
    "- **Libro**: *Deep Learning for Coders with Fastai & PyTorch* (2020)   \n",
    "  - Revolucionario por su enfoque práctico con Fastai (basado en PyTorch).  \n",
    "  - Cubre desde CNN hasta modelos generativos, con notebooks ejecutables.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc20aafb",
   "metadata": {},
   "source": [
    "\n",
    "### **3. Ian Goodfellow, Yoshua Bengio & Aaron Courville**  \n",
    "- **Libro**: *Deep Learning* (2ª ed., esperada en 2024)   \n",
    "  - Referencia clásica actualizada con avances en GANs, RL y ética en IA.  \n",
    "  - Ideal para fundamentos matemáticos rigurosos.  \n",
    "\n",
    "### **4. Lewis Tunstall, Leandro von Werra & Thomas Wolf**  \n",
    "- **Libro**: *Natural Language Processing with Transformers* (2022)   \n",
    "  - Enfoque en NLP moderno (BERT, GPT, etc.) usando Hugging Face.  \n",
    "  - Incluye código en PyTorch y TensorFlow.  \n",
    "\n",
    "### **5. Kevin Murphy**  \n",
    "- **Libro**: *Probabilistic Machine Learning: Advanced Topics* (2023)   \n",
    "  - Profundiza en modelos probabilísticos y Bayesian Deep Learning.  \n",
    "  - Complemento ideal para cursos avanzados.  \n",
    "\n",
    "### **6. Sebastian Raschka & Vahid Mirjalili**  \n",
    "- **Libro**: *Machine Learning with PyTorch and Scikit-Learn* (2022)   \n",
    "  - Actualización de su clásico, ahora integrando PyTorch.  \n",
    "  - Cubre desde fundamentos hasta arquitecturas profundas.  \n",
    "\n",
    "### **7. Andrew Trask**  \n",
    "- **Libro**: *Grokking Deep Learning* (2023)  \n",
    "  - Enfoque visual e intuitivo, ideal para estudiantes sin base matemática fuerte.  \n",
    "  - Implementaciones desde cero en Python.  \n",
    "\n",
    "### **8. Chip Huyen**  \n",
    "- **Libro**: *Designing Machine Learning Systems* (2022)   \n",
    "  - Enseña a llevar modelos a producción (MLOps), clave para docencia aplicada.  \n",
    "  - Incluye estudios de caso con TensorFlow Extended (TFX).  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb12aef",
   "metadata": {},
   "source": [
    "\n",
    "### **9. Alfredo Canziani & Yann LeCun**  \n",
    "- **Recurso**: *NYU Deep Learning Course* (2023, disponible en GitHub)  \n",
    "  - Materiales abiertos con PyTorch, enfocados en investigación y aplicaciones.  \n",
    "  - Incluye videos y ejercicios actualizados.  \n",
    "\n",
    "### **10. Aurélien Géron (actualización indirecta)**  \n",
    "- **Curso**: *TensorFlow Developer Certificate Program* (2023)  \n",
    "  - Aunque no es un libro, su contenido en Coursera actualiza prácticas con TF 2.x.  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a82b086",
   "metadata": {},
   "source": [
    "\n",
    "### **Criterios de selección**:  \n",
    "- **Enfoque práctico**: Todos incluyen código en Python (PyTorch/Keras/TensorFlow) .  \n",
    "- **Temas modernos**: Transformers, MLOps, modelos probabilísticos .  \n",
    "- **Niveles variados**: Desde introductorios (Fastai) hasta avanzados (Murphy).  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ee0fe1",
   "metadata": {},
   "source": [
    "\n",
    "### **Recomendación adicional**:  \n",
    "Para cursos introductorios, combina *Chollet* (Keras) con *Howard* (Fastai). Para avanzados, *Murphy* y *Tunstall* ofrecen profundidad teórica y técnica .  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49337cd1",
   "metadata": {},
   "source": [
    "\n",
    "Si necesitas detalles sobre un autor en particular o materiales complementarios (como datasets o notebooks), ¡avísame!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
