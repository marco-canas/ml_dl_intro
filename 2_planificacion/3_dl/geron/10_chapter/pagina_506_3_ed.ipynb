{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "220f4a92",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/marco-canas/ml_intro/blob/main/2_planificacion/3_dl/geron/10_chapter/pagina_506_3_ed.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://kaggle.com/kernels/welcome?src=https://github.com/marco-canas/ml_intro/blob/main/2_planificacion/3_dl/geron/10_chapter/pagina_506_3_ed.ipynb\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" /></a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a82726",
   "metadata": {},
   "source": [
    "Aquí tienes la traducción al español de las páginas **507 y 508** del libro *\"Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow\"* de Aurélien Géron, sección **\"Building a Regression MLP Using the Sequential API\"**:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61ce005",
   "metadata": {},
   "source": [
    "\n",
    "# **Construyendo un MLP de Regresión Usando la API Secuencial**  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252e4ff0",
   "metadata": {},
   "source": [
    "Volvamos al problema de la vivienda en California (*California housing*) y abordémoslo usando el mismo **MLP** (*Multi-Layer Perceptron*) que antes, con **3 capas ocultas de 50 neuronas cada una**, pero esta vez construyéndolo con Keras.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf182ab4",
   "metadata": {},
   "source": [
    "\n",
    "Usar la **API Secuencial** para construir, entrenar, evaluar y utilizar un MLP de regresión es muy similar a lo que hicimos para clasificación. Las principales diferencias en el siguiente ejemplo son:  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0072580",
   "metadata": {},
   "source": [
    "1. **Capa de salida**: Tiene una sola neurona (ya que solo predecimos un valor) y **no usa función de activación**.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229daafb",
   "metadata": {},
   "source": [
    "2. **Función de pérdida**: Error cuadrático medio (*mean squared error*, MSE). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eca6d66",
   "metadata": {},
   "source": [
    " \n",
    "3. **Métrica**: RMSE (*Root Mean Squared Error*).  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12003cac",
   "metadata": {},
   "source": [
    "4. **Optimizador**: Usamos **Adam**, igual que `MLPRegressor` de Scikit-Learn.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484eb9b0",
   "metadata": {},
   "source": [
    "\n",
    "Además, en este ejemplo:  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648b6680",
   "metadata": {},
   "source": [
    "- No necesitamos una capa `Flatten`.  \n",
    "- Usamos una capa de **Normalización** como primera capa: hace lo mismo que `StandardScaler` de Scikit-Learn, pero debe ajustarse a los datos de entrenamiento con su método `adapt()` antes de llamar a `fit()`. (Keras tiene otras capas de preprocesamiento, que se cubrirán en el Capítulo 13).  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fe2ac6",
   "metadata": {},
   "source": [
    "\n",
    "#### **Código Ejemplo**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9cb7d5",
   "metadata": {},
   "source": [
    "Aquí tienes las líneas de código específicas para:  \n",
    " \n",
    "* **obtener los datos, \n",
    "* dividirlos en predictores (X) y etiquetas (y)**, y \n",
    "* luego separarlos en conjuntos de **entrenamiento, validación y prueba**   \n",
    " \n",
    "para el ejemplo del MLP secuencial con Keras que tradujimos anteriormente:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a377d1e1",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### **1. Obtener el Dataset (California Housing)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04609324",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856b1665",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cargar el dataset completo\n",
    "housing = fetch_california_housing()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acde3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(housing) # La estructura es un objeto de tipo Bunch, similar a un diccionario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906796d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Separar predictores (X) y etiquetas (y)\n",
    "X, y = housing.data, housing.target  # X.shape = (20640, 8), y.shape = (20640,)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76233685",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# **2. Dividir en Train (70%), Validación (15%) y Test (15%)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204a05f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primera división: Train (70%) y Temporal (30%)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.3, \n",
    "    random_state=42  # Semilla para reproducibilidad\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d11bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Segunda división: Validación (15%) y Test (15%) del temporal\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(\n",
    "    X_temp, y_temp, \n",
    "    test_size=0.5,  # Divide el 30% en 15% validación y 15% test\n",
    "    random_state=42\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf2d09b",
   "metadata": {},
   "source": [
    "\n",
    "### **3. Verificación de las Dimensiones**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65073d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Train: {X_train.shape}, {y_train.shape}\")      # (14448, 8), (14448,)\n",
    "print(f\"Validación: {X_valid.shape}, {y_valid.shape}\")  # (3096, 8), (3096,)\n",
    "print(f\"Test: {X_test.shape}, {y_test.shape}\")          # (3096, 8), (3096,)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703d0306",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "### **Explicación Clave**:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ab5c7d",
   "metadata": {},
   "source": [
    "- **`random_state=42`**: Garantiza que la división sea reproducible (misma división en cada ejecución).  \n",
    "- **Proporciones**:  \n",
    "  - **70% entrenamiento**: Para aprender patrones.  \n",
    "  - **15% validación**: Para ajustar hiperparámetros y evitar overfitting.  \n",
    "  - **15% test**: Para evaluar el modelo final de manera imparcial.  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41347db1",
   "metadata": {},
   "source": [
    "\n",
    "### **Nota sobre Preprocesamiento**:\n",
    "El texto de Géron usa una **capa de Normalización de Keras** (no `StandardScaler`), por lo que no escalamos manualmente los datos. La capa se ajustará con `.adapt()` antes del entrenamiento:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d74092f",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_layer = tf.keras.layers.Normalization(input_shape=X_train.shape[1:])\n",
    "norm_layer.adapt(X_train)  # Ajuste a los datos de entrenamiento\n",
    "# Normalizar los conjuntos de datos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3cddff",
   "metadata": {},
   "source": [
    "\n",
    "# **¿Por qué esta división?**  \n",
    "- **Train**: Mayor porcentaje para que el modelo aprenda bien.  \n",
    "- **Validación**: Detecta overfitting durante el entrenamiento (ej.: con `validation_data` en `model.fit()`).  \n",
    "- **Test**: Simula datos nunca vistos para evaluar el rendimiento real.  \n",
    "- **Proporciones**:  \n",
    "  - **70% entrenamiento**: Para aprender patrones.  \n",
    "  - **15% validación**: Para ajustar hiperparámetros y evitar overfitting.  \n",
    "  - **15% test**: Para evaluar el modelo final de manera imparcial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a32be6",
   "metadata": {},
   "source": [
    "\n",
    "Si ejecutas este código antes del modelo secuencial de Géron, tendrás los datos listos para entrenar el MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d017f836",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Fijar semilla para reproducibilidad\n",
    "tf.random.set_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f107fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Capa de Normalización (equivalente a StandardScaler)\n",
    "norm_layer = tf.keras.layers.Normalization(input_shape=X_train.shape[1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8482a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape[1:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759016ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Modelo Secuencial\n",
    "model = tf.keras.Sequential([\n",
    "    norm_layer,\n",
    "    tf.keras.layers.Dense(50, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(50, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(50, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1)  # Sin activación para regresión\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d280203e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Optimizador y compilación\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "model.compile(loss=\"mse\", optimizer=optimizer, metrics=[\"RootMeanSquaredError\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f722ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%time \n",
    "# Ajustar normalización y entrenar\n",
    "norm_layer.adapt(X_train)\n",
    "history = model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8152f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluación y predicción\n",
    "mse_test, rmse_test = model.evaluate(X_test, y_test)\n",
    "X_new = X_test[:3]\n",
    "y_pred = model.predict(X_new)\n",
    "y_pred, y_test[:3]  # Comparar predicciones con etiquetas reales\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d75930",
   "metadata": {},
   "source": [
    "\n",
    "## **Nota Importante**:  \n",
    "La capa de `Normalization` aprende las **medias y desviaciones estándar** de las características al llamar a `adapt()`. Sin embargo, en el resumen del modelo, estos parámetros aparecen como **no entrenables** (*non-trainable*), porque no son afectados por el descenso de gradiente.  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2831b88f",
   "metadata": {},
   "source": [
    "\n",
    "### **Ventajas y Limitaciones de la API Secuencial**  \n",
    "Como ves, la **API Secuencial** es clara y sencilla. No obstante, aunque los modelos secuenciales son muy comunes, a veces es necesario construir redes con:  \n",
    "- Topologías más complejas.  \n",
    "- Múltiples entradas o salidas.  \n",
    "\n",
    "Para estos casos, Keras ofrece la **API Funcional** (que se explicará más adelante).  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07745ffd",
   "metadata": {},
   "source": [
    "\n",
    "### **Traducción de Términos Clave**:  \n",
    "| Inglés | Español |  \n",
    "|--------|---------|  \n",
    "| Hidden layers | Capas ocultas |  \n",
    "| Output layer | Capa de salida |  \n",
    "| Mean squared error (MSE) | Error cuadrático medio |  \n",
    "| Root Mean Squared Error (RMSE) | Raíz del error cuadrático medio |  \n",
    "| Adam optimizer | Optimizador Adam |  \n",
    "| Flatten layer | Capa de aplanamiento |  \n",
    "| Normalization layer | Capa de normalización |  \n",
    "| Non-trainable parameters | Parámetros no entrenables |  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203192ac",
   "metadata": {},
   "source": [
    "\n",
    "Esta traducción conserva el **tono técnico** del original y adapta los conceptos para hispanohablantes, manteniendo los términos clave en inglés entre paréntesis cuando es relevante. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9a9a28",
   "metadata": {},
   "source": [
    "# Práctica de codificación "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a2c045",
   "metadata": {},
   "source": [
    "Vamos a diseñar una **práctica de codificación paso a paso** basada en el texto de Géron, centrada en desarrollar un **MLP para regresión** con Keras (como en el ejemplo del libro), pero añadiendo desafíos para fortalecer tu **pensamiento computacional**. Aprenderás a:  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faff1c4c",
   "metadata": {},
   "source": [
    "\n",
    "1. **Preprocesar datos** (normalización).  \n",
    "2. **Construir y entrenar un modelo secuencial**.  \n",
    "3. **Evaluar y ajustar hiperparámetros**.  \n",
    "4. **Analizar resultados** (visualización de métricas).  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c8ff41",
   "metadata": {},
   "source": [
    "\n",
    "# **Práctica: MLP para Predecir Precios de Viviendas en California**  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c286f3",
   "metadata": {},
   "source": [
    "**Dataset**: Usaremos el mismo dataset que Géron (*California Housing*), disponible en `sklearn.datasets`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a62ff28",
   "metadata": {},
   "source": [
    "\n",
    "#### **Paso 1: Configuración del Entorno**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5875bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "# Se tarda alrededor de 9 segundos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1aa7f6f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 13 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# Cargar datos\n",
    "housing = fetch_california_housing()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dde6eab2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'frame', 'target_names', 'feature_names', 'DESCR'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing.keys() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "814451a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dividir en predictores y etiquetas \n",
    "X, y = housing.data, housing.target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6364feb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.526, 3.585, 3.521, ..., 0.923, 0.847, 0.894])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b1afa77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 8.01 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# Dividir en train (70%), validación (15%), test (15%)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323b5e35",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### **Paso 2: Preprocesamiento con Normalización**  \n",
    "Aquí compararemos dos enfoques:  \n",
    "- **Normalización con Keras** (como en Géron).  \n",
    "- **StandardScaler de Scikit-Learn** (para entender diferencias).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e42805f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marco\\Documentos\\investigacion\\ml_intro\\.venv\\Lib\\site-packages\\keras\\src\\layers\\preprocessing\\tf_data_layer.py:19: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Opción 1: Normalización con Keras\n",
    "norm_layer = tf.keras.layers.Normalization(input_shape=X_train.shape[1:])\n",
    "norm_layer.adapt(X_train)  # Ajuste a datos de entrenamiento\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8857b403",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Opción 2: StandardScaler de Scikit-Learn\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df10cf34",
   "metadata": {},
   "source": [
    "\n",
    "#### **Paso 3: Construcción del Modelo Secuencial**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ce902f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(normalization_layer=None):\n",
    "    model = tf.keras.Sequential()\n",
    "    if normalization_layer:\n",
    "        model.add(normalization_layer)  # Usar capa de Keras\n",
    "    model.add(tf.keras.layers.Dense(50, activation=\"relu\"))\n",
    "    model.add(tf.keras.layers.Dense(50, activation=\"relu\"))\n",
    "    model.add(tf.keras.layers.Dense(50, activation=\"relu\"))\n",
    "    model.add(tf.keras.layers.Dense(1))  # Salida lineal para regresión\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "    model.compile(loss=\"mse\", metrics=[\"RootMeanSquaredError\"])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80e91f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m452/452\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - RootMeanSquaredError: 1.0970 - loss: 1.2904 - val_RootMeanSquaredError: 0.6712 - val_loss: 0.4505\n",
      "Epoch 2/20\n",
      "\u001b[1m452/452\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - RootMeanSquaredError: 0.6346 - loss: 0.4028 - val_RootMeanSquaredError: 0.6139 - val_loss: 0.3768\n",
      "Epoch 3/20\n",
      "\u001b[1m452/452\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - RootMeanSquaredError: 0.6005 - loss: 0.3607 - val_RootMeanSquaredError: 0.6010 - val_loss: 0.3612\n",
      "Epoch 4/20\n",
      "\u001b[1m452/452\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - RootMeanSquaredError: 0.5880 - loss: 0.3458 - val_RootMeanSquaredError: 0.6031 - val_loss: 0.3637\n",
      "Epoch 5/20\n",
      "\u001b[1m452/452\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - RootMeanSquaredError: 0.5789 - loss: 0.3352 - val_RootMeanSquaredError: 0.5869 - val_loss: 0.3445\n",
      "Epoch 6/20\n",
      "\u001b[1m452/452\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - RootMeanSquaredError: 0.5690 - loss: 0.3238 - val_RootMeanSquaredError: 0.5710 - val_loss: 0.3260\n",
      "Epoch 7/20\n",
      "\u001b[1m452/452\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - RootMeanSquaredError: 0.5662 - loss: 0.3208 - val_RootMeanSquaredError: 0.5881 - val_loss: 0.3459\n",
      "Epoch 8/20\n",
      "\u001b[1m452/452\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - RootMeanSquaredError: 0.5722 - loss: 0.3275 - val_RootMeanSquaredError: 0.5761 - val_loss: 0.3319\n",
      "Epoch 9/20\n",
      "\u001b[1m452/452\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - RootMeanSquaredError: 0.5585 - loss: 0.3123 - val_RootMeanSquaredError: 0.5652 - val_loss: 0.3195\n",
      "Epoch 10/20\n",
      "\u001b[1m452/452\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - RootMeanSquaredError: 0.5981 - loss: 0.3605 - val_RootMeanSquaredError: 0.6423 - val_loss: 0.4126\n",
      "Epoch 11/20\n",
      "\u001b[1m452/452\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - RootMeanSquaredError: 0.5410 - loss: 0.2930 - val_RootMeanSquaredError: 0.6446 - val_loss: 0.4156\n",
      "Epoch 12/20\n",
      "\u001b[1m452/452\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - RootMeanSquaredError: 0.5541 - loss: 0.3075 - val_RootMeanSquaredError: 0.5688 - val_loss: 0.3236\n",
      "Epoch 13/20\n",
      "\u001b[1m452/452\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - RootMeanSquaredError: 0.5433 - loss: 0.2955 - val_RootMeanSquaredError: 0.5656 - val_loss: 0.3199\n",
      "Epoch 14/20\n",
      "\u001b[1m452/452\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - RootMeanSquaredError: 0.5318 - loss: 0.2830 - val_RootMeanSquaredError: 0.5557 - val_loss: 0.3088\n",
      "Epoch 15/20\n",
      "\u001b[1m452/452\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - RootMeanSquaredError: 0.5392 - loss: 0.2908 - val_RootMeanSquaredError: 0.5589 - val_loss: 0.3124\n",
      "Epoch 16/20\n",
      "\u001b[1m452/452\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - RootMeanSquaredError: 0.5253 - loss: 0.2761 - val_RootMeanSquaredError: 0.5829 - val_loss: 0.3398\n",
      "Epoch 17/20\n",
      "\u001b[1m452/452\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - RootMeanSquaredError: 0.5396 - loss: 0.2912 - val_RootMeanSquaredError: 0.5908 - val_loss: 0.3491\n",
      "Epoch 18/20\n",
      "\u001b[1m452/452\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - RootMeanSquaredError: 0.5610 - loss: 0.3165 - val_RootMeanSquaredError: 0.5854 - val_loss: 0.3427\n",
      "Epoch 19/20\n",
      "\u001b[1m452/452\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - RootMeanSquaredError: 0.5269 - loss: 0.2779 - val_RootMeanSquaredError: 0.5671 - val_loss: 0.3216\n",
      "Epoch 20/20\n",
      "\u001b[1m452/452\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - RootMeanSquaredError: 0.5285 - loss: 0.2794 - val_RootMeanSquaredError: 0.5746 - val_loss: 0.3302\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%time \n",
    "# Modelo con normalización de Keras\n",
    "model_keras = build_model(norm_layer)\n",
    "history_keras = model_keras.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e090e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m452/452\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - RootMeanSquaredError: 1.0611 - loss: 1.1991 - val_RootMeanSquaredError: 0.6837 - val_loss: 0.4674\n",
      "Epoch 2/20\n",
      "\u001b[1m452/452\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - RootMeanSquaredError: 0.6241 - loss: 0.3896 - val_RootMeanSquaredError: 0.6598 - val_loss: 0.4353\n",
      "Epoch 3/20\n",
      "\u001b[1m452/452\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - RootMeanSquaredError: 0.6041 - loss: 0.3650 - val_RootMeanSquaredError: 0.6136 - val_loss: 0.3765\n",
      "Epoch 4/20\n",
      "\u001b[1m452/452\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - RootMeanSquaredError: 0.5893 - loss: 0.3474 - val_RootMeanSquaredError: 0.5945 - val_loss: 0.3535\n",
      "Epoch 5/20\n",
      "\u001b[1m452/452\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - RootMeanSquaredError: 0.5802 - loss: 0.3368 - val_RootMeanSquaredError: 0.5814 - val_loss: 0.3380\n",
      "Epoch 6/20\n",
      "\u001b[1m452/452\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - RootMeanSquaredError: 0.5705 - loss: 0.3257 - val_RootMeanSquaredError: 0.5920 - val_loss: 0.3505\n",
      "Epoch 7/20\n",
      "\u001b[1m452/452\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - RootMeanSquaredError: 0.5604 - loss: 0.3142 - val_RootMeanSquaredError: 0.5773 - val_loss: 0.3333\n",
      "Epoch 8/20\n",
      "\u001b[1m452/452\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - RootMeanSquaredError: 0.5494 - loss: 0.3019 - val_RootMeanSquaredError: 0.5707 - val_loss: 0.3258\n",
      "Epoch 9/20\n",
      "\u001b[1m452/452\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - RootMeanSquaredError: 0.5633 - loss: 0.3176 - val_RootMeanSquaredError: 0.5554 - val_loss: 0.3084\n",
      "Epoch 10/20\n",
      "\u001b[1m452/452\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - RootMeanSquaredError: 0.5493 - loss: 0.3018 - val_RootMeanSquaredError: 0.5913 - val_loss: 0.3497\n",
      "Epoch 11/20\n",
      "\u001b[1m452/452\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - RootMeanSquaredError: 0.5455 - loss: 0.2978 - val_RootMeanSquaredError: 0.5851 - val_loss: 0.3423\n",
      "Epoch 12/20\n",
      "\u001b[1m452/452\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - RootMeanSquaredError: 0.5435 - loss: 0.2955 - val_RootMeanSquaredError: 0.6021 - val_loss: 0.3625\n",
      "Epoch 13/20\n",
      "\u001b[1m452/452\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - RootMeanSquaredError: 0.5388 - loss: 0.2904 - val_RootMeanSquaredError: 0.5501 - val_loss: 0.3026\n",
      "Epoch 14/20\n",
      "\u001b[1m452/452\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - RootMeanSquaredError: 0.5428 - loss: 0.2954 - val_RootMeanSquaredError: 0.5579 - val_loss: 0.3113\n",
      "Epoch 15/20\n",
      "\u001b[1m452/452\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - RootMeanSquaredError: 0.5274 - loss: 0.2783 - val_RootMeanSquaredError: 0.5517 - val_loss: 0.3044\n",
      "Epoch 16/20\n",
      "\u001b[1m452/452\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - RootMeanSquaredError: 0.5393 - loss: 0.2913 - val_RootMeanSquaredError: 0.5627 - val_loss: 0.3166\n",
      "Epoch 17/20\n",
      "\u001b[1m452/452\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - RootMeanSquaredError: 0.5354 - loss: 0.2867 - val_RootMeanSquaredError: 0.5669 - val_loss: 0.3214\n",
      "Epoch 18/20\n",
      "\u001b[1m452/452\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - RootMeanSquaredError: 0.5154 - loss: 0.2659 - val_RootMeanSquaredError: 0.5535 - val_loss: 0.3064\n",
      "Epoch 19/20\n",
      "\u001b[1m452/452\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - RootMeanSquaredError: 0.5149 - loss: 0.2654 - val_RootMeanSquaredError: 0.5459 - val_loss: 0.2980\n",
      "Epoch 20/20\n",
      "\u001b[1m452/452\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - RootMeanSquaredError: 0.5180 - loss: 0.2686 - val_RootMeanSquaredError: 0.5453 - val_loss: 0.2973\n",
      "CPU times: total: 41.5 s\n",
      "Wall time: 36 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# Modelo con StandardScaler\n",
    "model_scaler = build_model()\n",
    "history_scaler = model_scaler.fit(X_train_scaled, y_train, epochs=20, validation_data=(X_valid_scaled, y_valid))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebaacff9",
   "metadata": {},
   "source": [
    "\n",
    "#### **Paso 4: Evaluación y Visualización** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fa4209",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history_keras' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     12\u001b[39m     plt.savefig(\u001b[33mrf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mC:\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mUsers\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mmarco\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mDownloads\u001b[39m\u001b[33m\\\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtitle\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.png\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     13\u001b[39m     plt.show()\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m plot_loss(\u001b[43mhistory_keras\u001b[49m, \u001b[33m\"\u001b[39m\u001b[33mNormalización con Keras\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     16\u001b[39m plot_loss(history_scaler, \u001b[33m\"\u001b[39m\u001b[33mNormalización con StandardScaler\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'history_keras' is not defined"
     ]
    }
   ],
   "source": [
    "# Función para graficar curvas de aprendizaje\n",
    "def plot_loss(history, title):\n",
    "    plt.plot(history.history[\"loss\"], label=\"Train\")\n",
    "    plt.plot(history.history[\"val_loss\"], label=\"Validation\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"MSE\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(alpha = 0.3)\n",
    "    plt.xticks(np.arange(0, 21, 1))\n",
    "    plt.yticks(np.arange(0.20, 0.90, 0.05))\n",
    "    plt.savefig(rf\"C:\\Users\\marco\\Downloads\\{title}.png\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eac7c289",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history_keras' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m plot_loss(\u001b[43mhistory_keras\u001b[49m, \u001b[33m\"\u001b[39m\u001b[33mNormalización con Keras\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'history_keras' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "plot_loss(history_keras, \"Normalización con Keras\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3dcac73",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history_scaler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m plot_loss(\u001b[43mhistory_scaler\u001b[49m, \u001b[33m\"\u001b[39m\u001b[33mNormalización con StandardScaler\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'history_scaler' is not defined"
     ]
    }
   ],
   "source": [
    "plot_loss(history_scaler, \"Normalización con StandardScaler\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9ea02330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - RootMeanSquaredError: 0.5418 - loss: 0.2938\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - RootMeanSquaredError: 0.5217 - loss: 0.2724\n",
      "Keras Normalization - Test RMSE: 0.5486\n",
      "StandardScaler - Test RMSE: 0.5124\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluar en test\n",
    "mse_keras, rmse_keras = model_keras.evaluate(X_test, y_test)\n",
    "mse_scaler, rmse_scaler = model_scaler.evaluate(X_test_scaled, y_test)\n",
    "print(f\"Keras Normalization - Test RMSE: {rmse_keras:.4f}\")\n",
    "print(f\"StandardScaler - Test RMSE: {rmse_scaler:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446fc416",
   "metadata": {},
   "source": [
    "\n",
    "#### **Paso 5: Desafíos para Pensamiento Computacional**  \n",
    "1. **Experimenta con Arquitecturas**:  \n",
    "   - ¿Qué pasa si cambias el número de neuronas (ej.: 30 en lugar de 50)?  \n",
    "   - Prueba añadir una cuarta capa oculta.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a30c4ba",
   "metadata": {},
   "source": [
    "\n",
    "2. **Ajuste de Hiperparámetros**:  \n",
    "   - Modifica el `learning_rate` del optimizador Adam (prueba 1e-2, 1e-4).  \n",
    "   - Usa **early stopping** para evitar sobreajuste: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6ac73418",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m early_stopping = tf.keras.callbacks.EarlyStopping(patience=\u001b[32m5\u001b[39m, restore_best_weights=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m history = \u001b[43mmodel\u001b[49m.fit(..., callbacks=[early_stopping])\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    " \n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)\n",
    "history = model.fit(..., callbacks=[early_stopping])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea639132",
   "metadata": {},
   "source": [
    "\n",
    "3. **Compara Preprocesadores**:  \n",
    "   - ¿Cuál método de normalización da mejores resultados? ¿Por qué?  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7539d6c",
   "metadata": {},
   "source": [
    "\n",
    "4. **Predicciones Cualitativas**:  \n",
    "   - Imprime las primeras 5 predicciones del modelo y compáralas con los valores reales:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c3fa87ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step\n",
      "Predicciones: [0.85819227 1.1058774  3.7119164  1.2053517  1.7731631 ]\n",
      "Valores Reales: [1.    1.188 3.761 2.    0.952]\n"
     ]
    }
   ],
   "source": [
    "  \n",
    "y_pred = model_keras.predict(X_test[:5])\n",
    "print(\"Predicciones:\", y_pred.flatten())\n",
    "print(\"Valores Reales:\", y_test[:5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5190f2fb",
   "metadata": {},
   "source": [
    "\n",
    "### **Resultado Esperado**  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4a1b7a",
   "metadata": {},
   "source": [
    "- Aprenderás a **debuggear modelos**: Si el RMSE es muy alto, revisa si los datos están bien normalizados.  \n",
    "- Entenderás el impacto de **la arquitectura y el learning rate** en el entrenamiento.  \n",
    "- Visualizarás el **sobreajuste** (si el loss de validación sube en epochs altas).  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a383b5af",
   "metadata": {},
   "source": [
    "\n",
    "### **Conclusión**  \n",
    "Esta práctica refleja el **flujo de trabajo real en ML**:  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b29f628",
   "metadata": {},
   "source": [
    "1. Preprocesar datos → 2. Construir modelo → 3. Entrenar/Ajustar → 4. Evaluar.  \n",
    "**Tips adicionales**:  \n",
    "- Usa `model.summary()` para ver la estructura del modelo.  \n",
    "- Explora `tf.keras.utils.plot_model()` para visualizar la arquitectura.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adde6037",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtf\u001b[49m.keras.utils.plot_model(model_keras, show_shapes=\u001b[38;5;28;01mTrue\u001b[39;00m, to_file=\u001b[33m'\u001b[39m\u001b[33mmodel_keras.png\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.plot_model(model_keras, show_shapes=True, to_file='model_keras.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
